---
title: Surprisal analyses with complexity data
author: "Molly Lewis"
date: "`r Sys.Date()`"
output: 
  html_document:
  toc: true
number_sections: true
theme: cerulean
---
  
***
***
  
```{r setup, echo = F, warning = F, message = F}
rm(list = ls())

doSlow = FALSE
fs = 11 # set graphical params
ts = 11
rs = 5
ps1 = 3
ps2 = 4

library(knitr)
library(ggplot2)
library(langcog)
library(dplyr)
library(tidyr)
library(broom)
library(psych)
library(stringr)

source("useful_ML.R") 

opts_chunk$set(cache = F, warning = F, message = F)
```

#### Reviewer 1 comment: 
"*First, if this paper were written before the Piantadosi 2011 paper and the Mahowald 2013 papers, I could understand why the authors would feel that by controlling for frequency in experiment 9, they could get away from the Zipfian assumption that the main predictor of word length is frequency. By now the common assumption (e.g. Seyfarth 2014; Cognition) is that predictability, rather than frequency, should be the main determiner of length. For the first two series of experiments the authors may have felt that they could evade that issue by constructing novel objects, or objects for which no prior predictability of frequency accounts would apply, but that's a very narrow view of predictability-based accounts. For instance, in the artificial object experiment series, if number of parts predicts complexity, it is not far-fetched to assume that subjects do have some predictability assessment for the novel objects, based on a na√Øve P(object) = <PI>[P(parts)], which would make objects with more parts less predictable, everything else being equal (and the authors seem to be aware of that on page 5, yet maintain the argument for complexity). That is, to rule out a predictability-based explanation, the authors need to contrast their findings with a model that can assign probabilities to first-seen items. If that's what the authors mean by "complex", then it should be spelled out. This is not only an issue for the artificial objects. For instance, body part names seem not to correspond to how difficult it is to understand their action, but rather to how likely we are to see them: ears, eyes and tongue are arguably more complex than intestine, muscle and cartilage. For existing words in Experiment 9 the authors should control for mean predictability rather than frequency (preferably for both), and then use whatever residual effect there is in Experiment 10, not bare complexity. Otherwise they do not rule out the possibility that frequency and predictability account for the correlation, at least in the lower pearson r languages. Without clearer controls for predictability and frequency, the experiments in this paper only show that subjects expect language to match their experience in matching longer words with less predictable and less frequent items, which is still interesting, but perhaps shouldn't take 10 experiments to prove.*"

  
***
***

To address this, here we use the 2-gram surprisal measure calculated from the British National Corpus (BNC), and reported in the Piantadosi et al. (2011) paper. I'm not using the google books measure because I keep running in to an error when I try to process the bigrams. 

In a model predicting word length (in phonemes) with complexity, log frequency, and surprisal, complexity and log frequency are reliable predictors of word length, but not surprisal. 

I also added to the cross-linguistic figure the correlation between complexity and length partialing out surprisal. The reviewer suggests using the residual effect in Experiment 10, rather than partialing out surprisal, but this seems unncessarily complicated. Seems like the partial correlation is more consistent with the other analyses and is more straight-forward. What do you think?
  
***
***

Read in BNC data and complexity norms.
```{r}
bnc.2gram = read.table('../data/corpus/English-KNN-H-2.txt', header = T) %>%
  mutate(log.bnc.frequency = log(context.count)) %>%
  top_n(25000, abs(log.bnc.frequency)) # following Piantadosi, restrict to most frequent words

lf.data = read.csv("../data/corpus/english_complexity_norms.csv") %>%
  group_by(word) %>%
  select(-X, -workerid, -trial) %>%
  summarise_each(funs(mean)) %>%
  left_join(bnc.2gram, by="word")
```

Surprisals and frequencies for our 499 words are normally distributed
```{r}
ggplot(lf.data, aes(x=surprisal)) + 
    geom_histogram(fill = "black", alpha = .6 , binwidth = 1,  origin = -0.5) +
  xlab("Surprisal") +
  ggtitle('BNC Surprisal') +
  themeML

ggplot(lf.data, aes(x=log.bnc.frequency)) + 
    geom_histogram(fill = "black", alpha = .6 , binwidth = 1,  origin = -0.5) +
  xlab("BNC log frequency") +
  ggtitle('BNC log frequency') +
  themeML
```

Surprisal and complexity are correlated with each other (r = .28), and both are correlated with length (surprisal: r = .36, complexity: .67).
```{r, echo = F}
ggplot(lf.data, aes(x=surprisal, complexity)) + 
  geom_point() +
  geom_smooth(method = "lm") +
  ggtitle('BNC Surprisal and complexity') +
  annotate("text", x=2, y=4, 
            label=paste("r=",round(cor(lf.data$surprisal, lf.data$complexity,
                                      use = "complete"), 2)), col="red", size = rs) +
  themeML

ggplot(lf.data, aes(x=surprisal, mrc.phon)) + 
  geom_point() +
  geom_smooth(method = "lm") +
  ggtitle('BNC Surprisal and length') +
  annotate("text", x=2, y=7.1, 
           label=paste("r=",round(cor(lf.data$mrc.phon, lf.data$surprisal,
                                      use = "complete"), 2)), col="red") +
  themeML

ggplot(lf.data, aes(x=complexity, mrc.phon)) + 
  geom_point() +
  geom_smooth(method = "lm") +
    ggtitle('Complexity and length') +

  annotate("text", x=2, y=7.1, 
            label=paste("r=",round(cor(lf.data$mrc.phon, lf.data$complexity,
                                      use = "complete"), 2)), col="red") +
  themeML
```

Do Piantadosi analysis complexity words and all words (BIGRAM surprisals).
```{r, echo = F, fig.width = 11}
## complexity words
# get correlations
complexity.piantadosi.df = rbind(tidy(cor.test(lf.data$nchars,lf.data$log.bnc.frequency, method = "spearman")),
                                 tidy(cor.test(lf.data$nchars,lf.data$subt.log.freq, method = "spearman")),
                                 tidy(cor.test(lf.data$nchars, lf.data$surprisal, method = "spearman")),
                                 tidy(cor.test(lf.data$nchars, lf.data$complexity, method = "spearman")))
complexity.piantadosi.df$corr = c("freq.bnc", "freq.subt", "surprisal", "complexity")
complexity.piantadosi.df$corr <- factor(complexity.piantadosi.df$corr, 
                             levels = rev(levels(factor(complexity.piantadosi.df$corr))))
complexity.piantadosi.df$words = "complexity words"

# get partial correlations for length 
p.cor.surp.f.bnc = pcor.test(lf.data$nchars, lf.data$surprisal, lf.data$log.bnc.frequency, method = "spearman")$estimate
p.cor.surp.f.subt = pcor.test(lf.data$nchars, lf.data$surprisal, lf.data$subt.log.freq, method = "spearman")$estimate
p.cor.surp.c = pcor.test(lf.data$nchars, lf.data$surprisal, lf.data$complexity, method = "spearman")$estimate
p.cor.freq.bnc.s = pcor.test(lf.data$nchars, lf.data$log.bnc.frequency, lf.data$surprisal, method = "spearman")$estimate
p.cor.freq.subt.s = pcor.test(lf.data$nchars, lf.data$subt.log.freq, lf.data$surprisal, method = "spearman")$estimate
p.cor.freq.bnc.c = pcor.test(lf.data$nchars, lf.data$log.bnc.frequency, lf.data$complexity, method = "spearman")$estimate
p.cor.freq.subt.c = pcor.test(lf.data$nchars, lf.data$subt.log.freq, lf.data$complexity, method = "spearman")$estimate
p.cor.complex.s = pcor.test(lf.data$nchars, lf.data$complexity, lf.data$surprisal, method = "spearman")$estimate
p.cor.complex.f.bnc = pcor.test(lf.data$nchars, lf.data$complexity, lf.data$log.bnc.frequency, method = "spearman")$estimate
p.cor.complex.f.subt = pcor.test(lf.data$nchars, lf.data$complexity, lf.data$subt.log.freq, method = "spearman")$estimate

complexity.partials = data.frame(p.corrs = c(p.cor.surp.f.bnc, p.cor.surp.f.subt, p.cor.surp.c, p.cor.freq.bnc.s, p.cor.freq.subt.s,
                                             p.cor.freq.bnc.c, p.cor.freq.subt.c, p.cor.complex.s, p.cor.complex.f.bnc, p.cor.complex.f.subt), 
                                 corr = c("surprisal", "surprisal", "surprisal", "freq.bnc", "freq.subt",
                                          "freq.bnc", "freq.subt","complexity", "complexity", "complexity"),
                                 partialed = c("freq.bnc", "freq.subt","complexity", "surprisal", "surprisal",
                                               "complexity", "complexity", "surprisal",  "freq.bnc", "freq.subt"))
complexity.partials$words = "complexity words"

# all words
# merge in subt.lex.us frequency
freqs = read.table("../data/corpus/SUBTLEXus_corpus.txt", header=T)

bnc.2gram = bnc.2gram %>%
  left_join(freqs, by=c("word" = "Word")) %>%
  rename(subt.log.freq = Lg10WF)

# get correlations
full.piantadosi.df = rbind(tidy(cor.test(bnc.2gram$len, bnc.2gram$log.bnc.frequency, method = "spearman")),
                           tidy(cor.test(bnc.2gram$len, bnc.2gram$subt.log.freq, method = "spearman")),
                           tidy(cor.test(bnc.2gram$len, bnc.2gram$surprisal, method = "spearman")))
full.piantadosi.df$corr = c("freq.bnc", "freq.subt", "surprisal")
full.piantadosi.df$words = "all words"

# get partial correlations
full.p.cor.surp.f.bnc = pcor.test(bnc.2gram$len, bnc.2gram$surprisal, bnc.2gram$log.bnc.frequency, method = "spearman")$estimate
full.p.cor.surp.f.subt = pcor.test(bnc.2gram$len, bnc.2gram$surprisal, bnc.2gram$subt.log.freq, method = "spearman")$estimate
full.p.cor.freq.bnc.s = pcor.test(bnc.2gram$len, bnc.2gram$log.bnc.frequency,bnc.2gram$surprisal, method = "spearman")$estimate
full.p.cor.freq.subt.s = pcor.test(bnc.2gram$len, bnc.2gram$subt.log.freq,bnc.2gram$surprisal, method = "spearman")$estimate
full.partials = data.frame(p.corrs = c(full.p.cor.surp.f.bnc, full.p.cor.surp.f.subt, full.p.cor.freq.bnc.s, full.p.cor.freq.subt.s), 
                           corr =c("surprisal", "surprisal", "freq.bnc", "freq.subt"),
                           partialed = c("freq.bnc", "freq.subt", "surprisal", "surprisal"))
full.partials$words = "all words"

both.piantadosi.df = rbind(complexity.piantadosi.df, full.piantadosi.df)
both.partials.df = rbind(complexity.partials, full.partials)

both.piantadosi.df$corr  <- factor(both.piantadosi.df$corr, 
                                   c( "freq.bnc", "freq.subt", "surprisal", "complexity"))
both.partials.df$corr  <- factor(both.partials.df$corr, 
                                   c( "freq.bnc", "freq.subt", "surprisal", "complexity"))

ggplot(both.piantadosi.df) +
  geom_bar(stat = "identity", aes(y = abs(estimate), x = corr, fill = corr))+
  scale_fill_manual(values = c("blue", "lightblue", "red" ,"yellow"),guide = FALSE) +
  facet_grid(. ~ words,  scales = "free_x", space = "free_x") +
  geom_point(data=both.partials.df, 
             mapping=aes(x=corr, y=abs(p.corrs), 
                         shape = partialed), size=4) +
  ylab("absolute spearman's r") +
  scale_x_discrete(name = "", drop = T)  +
  themeML +
  theme(text = element_text(size=20))
```

# stats for piantadosi analysis
```{r}
paired.r(abs(cor(bnc.2gram$len, bnc.2gram$log.bnc.frequency, method = "spearman")), 
         cor(bnc.2gram$len, bnc.2gram$surprisal, method = "spearman"),n=length(bnc.2gram$len))
```

# stats for surpisal and complexity analysis
```{r}
pcor.test(lf.data$mrc.phon, lf.data$complexity,lf.data$surprisal)
summary(lm(mrc.phon ~ complexity + surprisal + log.bnc.frequency, lf.data))
```


Now, let's look cross-linguistically. 

Read in xling data and merge with English complexity norms
```{r, include = F}
xling = read.csv("../data/corpus/xling_lens.csv") %>%
  left_join(lf.data, by = c("ENGLISH" = "word")) %>%
  select(contains("len"), complexity, class, clx.morph, subt.log.freq, surprisal, -len)
```

Get correlations for length and complexity with for all 499 words with bootstrapped CIs
```{r, include = F}
if(doSlow) {
  c_l = data.frame (language = character(), lower.ci = numeric(0), 
                    corr = numeric(0), upper.ci = numeric(0))
  levels(c_l$language) = names(xling)
  
  # for each language, calculate correlation between length and complexity norm
  for (i in 1:length(lens)){
    c_l[i, 2:4] = boot.cor(xling[,i], xling[ ,"complexity"])
    c_l[i, "language"] = names(xling)[i]
  }
  write.csv(c_l, "../data/corpus/xling_corrs.csv")
}

c_l = read.csv("../data/corpus/xling_corrs.csv") %>% 
  select(-X)
```

Partial correlations (with frequency and surprisal)
```{r, include = F}
# frequency
cmat.p.f = partial.r(xling, c(1:80, which(names(xling) == "complexity")), 
                   which(names(xling) == "subt.log.freq"))
cxl.p.f = as.data.frame(cmat.p.f[which(row.names(cmat.p.f) == "complexity"), 1:80])
cxl.p.f$lang = row.names(cxl.p.f)
names(cxl.p.f) = c("p.corr.f", "lang")

# surprisal
cmat.p.surp = partial.r(xling, c(1:80, which(names(xling) == "complexity")), 
                   which(names(xling) == "surprisal"))
cxl.p.surp = as.data.frame(cmat.p.surp[which(row.names(cmat.p.surp) == "complexity"), 1:80])
cxl.p.surp$lang = row.names(cxl.p.surp)
names(cxl.p.surp) = c("p.corr.surp", "lang")

# merge in partials
c_l = c_l %>%
     left_join(cxl.p.f, by = c("language" = "lang")) %>%
     left_join(cxl.p.surp, by = c("language" = "lang"))
```

Get correlation for monomorphemic and open class subsets
```{r, include = F}
# monomorphemic
mono_cor = data.frame(language = character(), mono.cor = numeric(0))
levels(mono_cor$language) = names(xling)

for (i in 1:80){
  mono_cor[i, "mono.cor"] = cor(xling[xling$clx.morph == 1, i], 
                                xling[xling$clx.morph == 1,  "complexity"], 
                                use = "complete")
  mono_cor[i, "language"] = names(xling)[i]
}

# open class
open_cor = data.frame(language = character(), open.cor = numeric(0))
levels(open_cor$language) = names(xling)

for (i in 1:80){
  open_cor[i, "open.cor"] = cor(xling[xling$class == 1, i], 
                                xling[xling$class == 1, "complexity"], use = "complete")
  open_cor[i, "language"] = names(xling)[i]
}

c_l = c_l %>%
     left_join(mono_cor) %>%
     left_join(open_cor)
```

prep for plotting
```{r, include = F}
c_l$language=  as.character(tolower(lapply(str_split(c_l$language,"_"),
                                           function(x) {x[1]}))) # clean up names
c_l$language = reorder(c_l$language, c_l$corr, mean)
c_l$language= factor(c_l$language, levels = rev(levels(c_l$language)))
#c_l$language <- factor(c_l$language, levels =  rev(as.character(c_l$language))) 

# add check information
coded_languages = c("english", "spanish", "welsh", "vietnamese", "russian", "portuguese", 
                    "persian", "bosnian", "french", "hebrew", "italian", "korean", "polish")
c_l$checked = as.factor(ifelse(is.element(c_l$language, coded_languages), "yes", "no"))

# fix several labels
levels(c_l$language)[levels(c_l$language)=="haitian.creole"] <- "haitian creole"
levels(c_l$language)[levels(c_l$language)=="espernto"] <- "esperanto"
levels(c_l$language)[levels(c_l$language)=="cebuana"] <- "cebuano"
```

We counted the number of unicode characters for each translation. Variability in word length within languages was positively correlated with complexity ratings. Below the correlation coefficients are plotted for each language. Red bars indicate languages where the accuracy was checked by a native speaker and pink bars indicate unchecked languages. The dashed line indicates the grand mean correlation across languages. Full circles indicate the correlation between complexity and length, partialling out log spoken frequency in English. Empty circles show the correlation between complexity and length, partialling out surprisal. Triangles indicate the correlation between complexity and length for the subset of words that are monomorphemic in English. Squares indicate the correlation between complexity and length for the subset of open class words. 

```{r, echo = F, fig.width = 2.5*ps2, fig.height = 1.5*ps2,  include = F}

#pdf("figure/FIG_3.pdf", height = 6, width = 12)
#ggplot(c_l, aes(language, corr, fill = "grey")) + 
ggplot(c_l, aes(language, corr, fill = checked)) + 
  geom_bar(stat = "identity") + 
  geom_linerange(aes(ymax=upper.ci, ymin=lower.ci)) +
  geom_point(data=c_l, mapping=aes(x=language, y=p.corr.f), size=2, shape = 16) +
  geom_point(data=c_l, mapping=aes(x=language, y=p.corr.surp), size=2, shape = 1) +
  geom_point(data=c_l, mapping=aes(x=language, y=mono.cor), size=2, shape = 17) +
  geom_point(data=c_l, mapping=aes(x=language, y=open.cor), size=2, shape = 15) +
  geom_hline(aes(yintercept=mean(c_l$corr)), lty=2) +
  geom_hline(aes(yintercept= 0), lty=1) +
  ylab("Pearson's r") +
  xlab("Language") + 
  ggtitle("Correlation between complexity ratings and length") +
  themeML + 
  theme(text = element_text(size=fs-5),
        legend.position="none",
        axis.text.x = element_text(angle = 90, hjust = 1, vjust = .5)) +
  scale_fill_manual(values=c("pink", "red")) +
  scale_y_continuous(limits = c(-.08, .75), expand = c(0,.007)) 
#dev.off()
```

```{r, include = F}
# grand mean correlation between complexity and length
mean(c_l$corr) 

# grand mean correlation between complexity and length, checked languages only
mean(c_l[c_l$checked == "yes", "corr"]) 

# grand mean correlation between complexity and length, mono-morphemic only
mean(c_l$mono.cor)

# grand mean correlation between complexity and length, open class only
mean(c_l$open.cor)

# grand mean correlation between complexity and length, partialling out frequency
mean(c_l$p.corr.f)

# grand mean correlation between complexity and length, partialling out frequency
mean(c_l$p.corr.surp)
```

