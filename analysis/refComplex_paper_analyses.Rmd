Referential Complexity Analyses
====
M. Lewis 
====
`r as.character(format(Sys.Date(), format="%B %d, %Y"))`
====

***
***

<h2> Analyses:<h2>

1. [Geons](#geons) <br/> 
  (A) [Norms](#1a) <br/> 
  (B) [Mappping task](#1b)<br/>
  (\(C\)) [Mapping task control (random syllables)](#1c) <br/> 

2. [Novel real objects](#novelRealObjs)<br/> 
  (A) [Norms](#2a)<br/> 
  (B) [Mappping task](#2b) <br/> 
  (\(C\)) [Mapping task control (random syllables)](#2c) <br/> 
  (D) [Production task](#2d) <br/> 

3. [Natural language](#xling) <br/> 
  (A) [English norms](#3a) <br/>
  (B) [Xling translation accuracy](#3b) <br/> 
  (\(C\)) [Xling correlation between lengths and complexity](#3c) <br/> 
  
***
***

#### SET GLOBAL VARIABLES
```{r global_vars, include=TRUE}
rm(list=ls())

whichSubjRemove = 'keepAll' # remove repeat subjects? ('keepAll', 'repeatSubj', 'withinRepeatSubj')
processNorms = FALSE # process norms or load norms? 
savePlots = FALSE # save plots to pdf?
doSlow = FALSE # do time-consuming pre-processing steps?
```

#### LOAD PACKAGES, FUNCTIONS, AND REPEAT SUBJ DATA FILE
```{r load_stuff, include = FALSE}

# load functions and libraries
source("/Documents/GRADUATE_SCHOOL/Ranalysis/useful.R")
library('psych')
library('boot')

cor.mtest <- function(mat, conf.level = 0.95) {
  mat <- as.matrix(mat)
  n <- ncol(mat)
  p.mat <- lowCI.mat <- uppCI.mat <- matrix(NA, n, n)
  diag(p.mat) <- 0
  diag(lowCI.mat) <- diag(uppCI.mat) <- 1
  for (i in 1:(n - 1)) {
    for (j in (i + 1):n) {
      tmp <- cor.test(mat[, i], mat[, j], conf.level = conf.level)
      p.mat[i, j] <- p.mat[j, i] <- tmp$p.value
      lowCI.mat[i, j] <- lowCI.mat[j, i] <- tmp$conf.int[1]
      uppCI.mat[i, j] <- uppCI.mat[j, i] <- tmp$conf.int[2]
      }
    }
  return(list(p.mat, lowCI.mat, uppCI.mat))
  }

# bootstrap CIs on CI (http://dodonovs.com/R/002-r.htm)
boot.cor = function(x, y, n=5000, p=0.95, method="pearson"){ 
  w = length(x); x.r = x; y.r = y 
  sm = 1:w; cor.b = 1:n 
  for (k in 1:n){ 
    s = sample(sm, w, replace = T) 
    for (i in 1:w) 
      { 
      x.r[i] = x[s[i]] 
      y.r[i] = y[s[i]] 
      } 
    cor.b[k] = cor(x.r, y.r, use = "pairwise", method) 
    } 
  cor.b = sort(cor.b); a = round(n*(1-p)/2,0); b = round(n*(p+1)/2,0) 
  vec = c(cor.b[a+1], cor(x, y, use = "pairwise", method), cor.b[b-1]) 
  n.r = c("value"); n.c = c("lower_bound", "correlation", "upper_bound") 
  matrix(vec,1,3,dimnames = list(n.r,n.c)) 
  }

# partial corerlation
bm.partial<-function(x,y,z) {
  round((cor(x,y, use="complete.obs")-cor(x,z, use="complete.obs")*cor(y,z, use="complete.obs"))/
                                     sqrt((1-cor(x,z, use="complete.obs")^2)*(1-cor(y,z, use="complete.obs")^2)),4)
  }

# effect size for forced choice tasks
d.fc <- function(d) {
  cond <- all(intersect(levels(d$langCondition),c("\"long\"" , "\"short\"")) == c("\"long\"" , "\"short\""))
  
  # http://www.meta-analysis.com/downloads/Meta-analysis%20Converting%20among%20effect%20sizes.pdf
  if (cond) {
    d<- d[d$langCondition == "\"long\"" | d$langCondition ==  "\"short\"",]
    d <- droplevels(d)
    
    #use odds ratio to calculate d
    ns = table(d$langCondition, d$responseValue)
    or = (ns[1]*ns[4])/(ns[2]*ns[3]) 
    cf = sqrt(3)/pi
    effect_size = log(or) * cf #calculate d
    
    # caluclate 95 CI
    se = sqrt((1/ns[1]) + (1/ns[2]) + (1/ns[3]) + (1/ns[4])) # calculate se of log odds ratio
    d_se = se * (3/(pi^2)) # caclulate se of 
    d_err = d_se*1.96
    
    cill = effect_size - d_err
    ciul = effect_size + d_err
    rt.Mratio = mean(d$rt.ratio, na.rm = TRUE)
    c.Mratio = mean(d$c.ratio, na.rm = TRUE)
    
    es <- data.frame(effect_size=effect_size,
                     cill = cill,
                     ciul = ciul,
                     rt.Mratio = rt.Mratio,
                     c.Mratio = c.Mratio)
    }
  return (es)
  }

# proportions and CIs for forced choice tasks
p.fc <- function(d){ 
  category = "\"complex\""
  conditions =  c("\"one\"" , "\"three\"", "\"five\"")
  d$response = d$responseValue
  
  # get proportions
  complex_proport_c = sum(d$response==category)/length(d$response)
  
  if (!is.na(complex_proport_c)) { 
    # Bootstrap across subjects proportion responses for each category
    b <- boot(d$response, function(u,i) table(u[i])[category]/length(u), R = 1000) 
    ci <- boot.ci(b, type =  "basic")  
    ciwl = ci$basic[4]
    ciwu = ci$basic[5]
    
    es <- data.frame(p_complex = complex_proport_c,
                     ciwl = ciwl,
                     ciul = ciwu,
                     n=length(d$workerid))
    
  } else {
    es <- data.frame(p_c = NA,
                     ciwl = NA,
                     ciul = NA,
                     n=NA)
    
  }
  return (es)
}

# read in repeat subject file
setwd('/Documents/GRADUATE_SCHOOL/Projects/ref_complex/Papers/RC/analysis/')
dups = read.csv("data/all_workerids.csv")
```

***
***

<a name="geons"/>

## (1) Geons

<a name="1a"/>

### (A) Norms [(Complexity Task)][task34] [(RT task)][task35]
### (1) Complexity Norms
### *get data into long form*
```{r geon_complexity_norms, warning = FALSE}
if (processNorms) {
  # read in data
  d <- read.csv("data/RefComplex34.results",sep="\t",header=TRUE)
  
  # melt
  md <- melt(d,id.vars=c("workerid"),measure.vars=names(d)[grepl("obj",names(d))])
  md$trial <- matrix(lapply(str_split(md$variable,"_"),function(x) {x[2]}))
  md$obj <- unlist(matrix(lapply(str_split(md$value,"j"),function(x) {x[2]})))
  md$obj <- unlist(matrix(lapply(str_split(md$obj,".p"),function(x) {x[1]})))
  md$complexityLevel <- unlist(matrix(lapply(str_split(md$obj,"-"),function(x) {x[1]})))
  md$objID <- unlist(matrix(lapply(str_split(md$obj,"-"),function(x) {x[2]})))
  md$value <- NULL; md$variable <- NULL
  md = md[(md$trial != 0 & md$trial != 1),] # remove ball and circuit
  
  # get rating info
  mdr <- melt(d,id.vars=c("workerid"),measure.vars=names(d)[grepl("rating",names(d))])
  mdr$trial <- matrix(lapply(str_split(mdr$variable,"_"),function(x) {x[2]}))
  mdr$variable <- NULL
  
  # merge together based on trial and workerid
  m = merge(md,mdr,by=c("workerid","trial"))
  m$value <- as.numeric(as.character(m$value))
  
  # get norms by objects
  ms_all <- aggregate(value ~ obj, data=m, mean)
  ms_all$cih <- aggregate(value ~ obj, data=m, ci.high)$value
  ms_all$cil <- aggregate(value ~ obj, data=m, ci.low)$value
  
  names(ms_all)[2] = "meanRating"
  ms_all <- ms_all[order(ms_all$meanRating),]

  # save complexity by item
  write.csv(ms_all, "data/complicatedNormsGeons_BYITEM.csv")
  } 

cg_norms = read.csv("data/complicatedNormsGeons_BYITEM.csv")
```

### *plot complexity norm by condition*
```{r}
  # remove quotes from norms
  cg_norms$obj <- as.factor(as.numeric(gsub("[[:punct:]]", "", cg_norms$obj)))
  cg_norms$obj_class = as.numeric(substr(cg_norms$obj, 1, 1))
  cg_norms$obj_item = as.numeric(substr(cg_norms$obj, 2, 2))

  ms <- aggregate(meanRating  ~ obj_class, data=cg_norms, mean)
  ms$cih <- aggregate(meanRating  ~ obj_class, data=cg_norms, ci.high)$meanRating
  ms$cil <- aggregate(meanRating  ~ obj_class, data=cg_norms, ci.low)$meanRating
    
  # plot
  ggplot(ms, aes(y=meanRating, x=as.numeric(obj_class))) +
    geom_errorbar(data=ms, mapping=aes(x=obj_class, ymax = meanRating+cih, 
                                       ymin=meanRating-cil), width=0.2, size=1, color="black") + 
    geom_point(data=ms, mapping=aes(x=obj_class, y=meanRating), size=6)  +
    geom_line() +
    xlab("Object Condition") +
    ylab("Complexity rating") +
    ggtitle("complexity norm vs. number of geons")
```

### *get complexity norm and condition correlation*
```{r}
# make object class numeric
cg_norms$obj_class = as.numeric(cg_norms$obj_class) 

# correlation between num geons and complexity
cor.test(cg_norms$obj_class, cg_norms$meanRating)
```


### (2) RT Norms 
### *get data into long form (accuracy and RT df) and do exclusions*
```{r geon_rt_norms, warning= FALSE}
if (processNorms) {
  # read in data
  raw <- read.csv("data/RefComplex37.results",sep="\t",header=TRUE)
  
  # remove repeat subjects?
  if (whichSubjRemove == 'removeRepeatSubj') {
    raw = merge(raw, dups, by=c("hitid","workerid"))
    raw = raw[!raw$repeatSubj,]
    } else if (whichSubjRemove == 'withinRepeatSubj') {
      raw = merge(raw, dups, by=c("hitid","workerid"))
      raw = raw[!raw$withinRepeatSubj,]
      }
  
  # look at accuracy, and exclude those below chance
  boxplot(raw$Answer.correct, main = "distribution before accuracy exclusions")
  raw = raw[raw$Answer.correct > 20, ]
  
  # prep accuracy and RT dataframes
  # accuracy data frame
  # melt
  n <- names(raw)
  cols = c( n[grepl("test",n)])
  mda <- melt(raw,id.vars=c("workerid"), measure.vars=cols,na.rm=TRUE)
  mda$trial <-as.numeric(matrix(lapply(str_split(mda$variable,"_"),function(x) {x[3]})))
  mda$var1 <- matrix(lapply(str_split(mda$variable,"_"),function(x) {x[1]}))
  mda$var2 <- matrix(lapply(str_split(mda$variable,"_"),function(x) {x[2]}))
  mda$var <- paste(mda$var1, mda$var2, sep = "_")
  mda$variable <- NULL; mda$var1 <- NULL; mda$var2 <- NULL
  
  mda$seq <- with(mda, ave(value, workerid, var, FUN = seq_along))
  da = dcast(workerid + seq + trial ~ var, data = mda, value.var = "value")
  da$seq <- NULL
  
  da=da[!is.na(da$trial),]
  
  da$Answer.test_answer = as.factor(da$Answer.test_answer)
  da$Answer.test_answerEval = as.factor(da$Answer.test_answerEval)
  da$Answer.test_image  = as.factor(da$Answer.test_image)
  
  # look at memory performance (correct: CR or hit)
  numCR = length(which(da$Answer.test_answerEval == "\"CR\""))
  numH = length(which(da$Answer.test_answerEval == "\"H\""))

  correct = (numCR + numH)/ dim(da)[1]
  print(paste("percent correct:", round(correct,2)))

  # get obj condition variable
  da$test_image2 <- as.character(matrix(lapply(str_split(da$Answer.test_image,"j"), function(x) {x[2]})))
  da$objCondition <- as.factor(as.character(matrix(lapply(str_split(da$test_image2,"-"), function(x) {x[1]}))))
  da$objItem <- as.character(matrix(lapply(str_split(da$test_image2,"-"), function(x) {x[2]})))
  da$objItem<- as.factor(as.numeric(gsub("[[:punct:]]", "", da$objItem)))
  
  # RT dataframe
  # melt
  n <- names(raw)
  cols = c(n[grepl("train", n)])
  cols = cols[1:40]
  md <- melt(raw, id.vars=c("workerid"), measure.vars=cols)
  md$trial <- as.numeric(matrix(lapply(str_split(md$variable,"_"),function(x) {x[3]})))
  md$var1 <- matrix(lapply(str_split(md$variable,"_"),function(x) {x[1]}))
  md$var2 <- matrix(lapply(str_split(md$variable,"_"),function(x) {x[2]}))
  md$var <- paste(md$var1, md$var2, sep = "_")
  md$variable <- NULL; md$var1 <- NULL; md$var2 <- NULL
  
  md$seq <- with(md, ave(value, workerid, var, FUN = seq_along))
  d = dcast(workerid + seq + trial ~ var, data = md, value.var = "value")
  d$Answer.train_NA <- NULL; d$seq <- NULL
  
  d=d[!is.na(d$trial),]
  d=d[!is.na(d$Answer.train_image),]
  d=d[!is.na(d$Answer.train_rt),]
  d=d[d$Answer.train_image != "undefined",]
  d=d[d$Answer.train_image != "",]
  
  d$Answer.train_rt = as.numeric(d$Answer.train_rt)
  
  # exclude outlier 2 standard deviations above and below mean (in log space)
  d$log.rt = log(d$Answer.train_rt)
  total = dim(d)[1]
  sd2 = 2*sd(d$log.rt)
  d = d[(d$log.rt > (mean(d$log.rt) - sd2)) & (d$log.rt < (mean(d$log.rt) + sd2)),]
  print(paste("percent trimmed:" , round((total - dim(d)[1])/total,2)))
  hist(d$log.rt)
  
  # get obj condition
  d = d[d$trial > 1,] # exclude anchors (not interesting because order not randomized)
  d$train_image2 <- as.character(matrix(lapply(str_split(d$Answer.train_image,"j"), function(x) {x[2]})))
  d$objCondition <- as.factor(as.character(matrix(lapply(str_split(d$train_image2,"-"), function(x) {x[1]}))))
  d$objItem <- as.character(matrix(lapply(str_split(d$train_image2,"-"), function(x) {x[2]})))
  d$objItem<- as.factor(as.numeric(gsub("[[:punct:]]", "", d$objItem)))
  d$obj <- paste("\"", d$objCondition, "-" ,d$objItem,"\"", sep = "" )
  
  # rt by item
  ms_all <- aggregate(log.rt  ~ obj, data=d, mean)
  ms_all$cih <- aggregate(log.rt ~ obj, data=d, ci.high)$log.rt
  ms_all$cil <- aggregate(log.rt ~ obj, data=d, ci.low)$log.rt
  
  # save RT by item
  write.csv(ms_all, "data/rtNormsGeons_BYITEM.csv")
  } 

rg_norms = read.csv("data/rtNormsGeons_BYITEM.csv")
```

###  *plot RT by condition*
```{r}
  rg_norms$obj <- as.factor(as.numeric(gsub("[[:punct:]]", "", rg_norms$obj)))
  rg_norms$obj_class = substr(rg_norms$obj, 1, 1)
  rg_norms$obj_item = substr(rg_norms$obj, 2, 2)

  ms <- aggregate(log.rt  ~ obj_class, data=rg_norms, mean)
  ms$cih <- aggregate(log.rt  ~ obj_class, data=rg_norms, ci.high)$log.rt
  ms$cil <- aggregate(log.rt  ~ obj_class, data=rg_norms, ci.low)$log.rt
    
  # plot
  ggplot(ms, aes(y=log.rt, x=as.numeric(obj_class))) +
    geom_errorbar(data=ms, mapping=aes(x=obj_class, ymax = log.rt+cih, 
                                       ymin=log.rt-cil), width=0.2, size=1, color="black") + 
    geom_point(data=ms, mapping=aes(x=obj_class, y=log.rt), size=6)  +
    geom_line() +
    xlab("Object Condition") +
    ylab("Log RT (ms)") + 
    ggtitle("RT vs. number of geons")
```

### *get RT correlation with condition*
```{r}
# make object class numeric
rg_norms$obj_class = as.numeric(rg_norms$obj_class) 

# correlation between num geons and complexity
cor.test(rg_norms$obj_class, rg_norms$log.rt)
```

### *get correlation between between complexity rating and RT*
```{r}
index <- match(cg_norms$obj, rg_norms$obj)
cg_norms$rt_meanRating <- rg_norms$log.rt[index]

cor.test(cg_norms$rt_meanRating, cg_norms$meanRating)
```

<a name="1b"/>

### (B) Mapping task [(Task)][task37] 
### *get data into long form*
```{r geon_mapping, warning = FALSE}
if (doSlow){
  # read in data
  d <- read.csv("data/RefComplex38.results",sep="\t",header=TRUE)
  
  # remove repeat subjects?
  if (whichSubjRemove == 'removeRepeatSubj') {
    d = merge(d, dups, by=c("hitid","workerid"))
    d = d[!d$repeatSubj,]
    } else if (whichSubjRemove == 'withinRepeatSubj') {
      d = merge(d, dups, by=c("hitid","workerid"))
      d = d[!d$withinRepeatSubj,]
      }
  
  # melt
  md <- melt(d,id.vars=c("workerid"),measure.vars=c(names(d)[c(grepl("_",names(d)))]))
  md$trial <- matrix(lapply(str_split(md$variable,"_"),function(x) {x[2]}))
  md$variable <- as.character(matrix(lapply(str_split(md$variable,"_"),function(x) {x[1]})))
  md$variable <- matrix(lapply(str_split(md$variable,"Answer."),function(x) {x[2]}))
  
  md$variable <- as.factor(as.character(md$variable))
  md$trial <- as.factor(as.character(md$trial))
  md$value <- as.factor(as.character(md$value))
  md$workerid <- as.factor(as.character(md$workerid))
  
  md$seq <- with(md, ave(value, workerid, variable, trial, FUN = seq_along))
  dc <- dcast(workerid + seq + trial ~ variable, data = md, value.var = "value")
  dc$seq <- NULL
  
  # make everything factors
  dc$criticalComplicated  <- as.factor(as.numeric(gsub("[[:punct:]]", "", dc$criticalComplicated))) #strip punctuations
  dc$criticalSimple  <- as.factor(as.numeric(gsub("[[:punct:]]", "", dc$criticalSimple))) #strip punctuations
  dc$langCondition <- as.factor(dc$langCondition)
  dc$objCondition <- as.factor(dc$objCondition)
  dc$response <- as.factor(dc$response)
  dc$responseSide <- as.factor(dc$responseSide)
  dc$responseValue  <- as.factor(dc$responseValue)
  dc$word <- as.factor(dc$word)
  
  # merge in norms
  # complicated
  index <- match(dc$criticalSimple, cg_norms$obj)
  dc$criticalSimple_c.norms <- cg_norms$meanRating[index]
  index <- match(dc$criticalComplicated,cg_norms$obj)
  dc$criticalComplicated_c.norms <- cg_norms$meanRating[index]
  
  # rt
  index <- match(dc$criticalSimple, rg_norms$obj)
  dc$criticalSimple_rt.norms <- rg_norms$log.rt [index]
  index <- match(dc$criticalComplicated, rg_norms$obj)
  dc$criticalComplicated_rt.norms <- rg_norms$log.rt [index]
  
  dc$c.ratio = dc$criticalSimple_c.norms/dc$criticalComplicated_c.norms
  dc$rt.ratio = dc$criticalSimple_rt.norms/dc$criticalComplicated_rt.norms
  
  write.csv(dc, "data/RC38_long.csv")
  }

dc <- read.csv('data/RC38_long.csv')
```

### *get effect sizes*
```{r}
de <- ddply(dc, .(objCondition), function (d) {d.fc(d)})
```

### *get obj conditions*
```{r}
de$cond1 <- as.factor(unlist(matrix(lapply(str_split(de$objCondition ,"-"),function(x) {x[1]}))))
de$cond2 <- as.factor(unlist(matrix(lapply(str_split(de$objCondition ,"-"),function(x) {x[2]}))))
de$cond1<- as.factor(gsub("[[:punct:]]", "", de$cond1))
de$cond2<- as.numeric(gsub("[[:punct:]]", "", de$cond2))
de$objCondition2 = paste(de$cond1, "/", de$cond2, sep = "")
```

### *plot complexity ratios as a function of condition*
```{r, warning = FALSE}
# set graphical params
fs = 15
rs = 8

# complexity ratio
geon_c_plot = ggplot(de, aes(y=effect_size, x=c.Mratio)) +
  geom_pointrange( aes(ymax = cill, ymin=ciul))+
  geom_hline(yintercept=0,lty=2) +
  stat_smooth(method="lm") +
  ylab("Effect Size (Cohen's d)") +
  xlab("Complexity Rating Ratio") + 
  theme(text = element_text(size=fs))  +
  scale_y_continuous(limits = c(-.33, .66)) +
  scale_x_continuous(limits = c(.25, 1.29)) +
  annotate("text", x=1.15, y=.5, color = "red", size = rs,
           label=paste("r=",round(cor(de$effect_size, de$c.Mratio), 2))) +
  theme(plot.background = element_blank(),
  panel.grid.major = element_blank(),
  panel.grid.minor = element_blank(),
  panel.border = element_blank(),
  axis.line = element_line(color = 'black')) 

# rt ratio
geon_rt_plot = ggplot(de, aes(y=effect_size, x=rt.Mratio)) +
  geom_pointrange( aes(ymax = cill, ymin=ciul))+
  geom_hline(yintercept=0,lty=2) +
  stat_smooth(method="lm") +
  ylab("Effect Size (Cohen's d)") +
  xlab("Reaction Time Ratio") + 
  theme(text = element_text(size=fs)) +  
  scale_y_continuous(limits = c(-.33, .66)) +
  scale_x_continuous(limits = c(.949, 1.005)) +
  annotate("text", x=.997, y=.5, color = "red", size = rs,
           label=paste("r=",round(cor(de$effect_size, de$rt.Mratio), 2))) +
  theme(plot.background = element_blank(),
  panel.grid.major = element_blank(),
  panel.grid.minor = element_blank(), 
  panel.border = element_blank(),
  axis.line = element_line(color = 'black')) 

if (savePlots){ 
  pdf("figure/geon.pdf", height = 6, width = 12)
  multiplot(geon_c_plot, geon_rt_plot, cols = 2)
  dev.off()
}else {
  multiplot(geon_c_plot, cols = 1)
  multiplot(geon_rt_plot, cols = 1)
}
```

###  *correlations between norms and effect sizes*
```{r}
cor.test(de$c.Mratio, de$effect_size)
cor.test(de$rt.Mratio, de$effect_size)
```
<a name="1c"/>

### (\(C\)) Mapping task control (random syllables)[(Task)][task40]
### *read data*

```{r geons_random_syllables, warning = TRUE, message = TRUE, output = TRUE}
d <- read.csv("data/RefComplex40.results",sep="\t",header=TRUE)

if (whichSubjRemove == 'removeRepeatSubj') { # FIX THIS 
  d = merge(d, dups, by=c("hitid","workerid"))
  d = d[!d$repeatSubj,]
  } else if (whichSubjRemove == 'withinRepeatSubj') {
    d = merge(d, dups, by=c("hitid","workerid"))
    d = d[!d$withinRepeatSubj,]
    }
```

### *get in long form and make everything factors*
```{r, warning = FALSE, message = FALSE, output = FALSE}
#get trial info
md <- melt(d,id.vars=c("workerid"),measure.vars=c(names(d)[c(grepl("_",names(d)))]))
md$trial <- matrix(lapply(str_split(md$variable,"_"),function(x) {x[2]}))
md$variable <- as.character(matrix(lapply(str_split(md$variable,"_"),function(x) {x[1]})))
md$variable <- matrix(lapply(str_split(md$variable,"Answer."),function(x) {x[2]}))

md$variable <- as.factor(as.character(md$variable))
md$trial <- as.factor(as.character(md$trial))
md$value <- as.factor(as.character(md$value))
md$workerid <- as.factor(as.character(md$workerid))

md$seq <- with(md, ave(value, workerid,  variable, trial, FUN = seq_along))
dc = dcast(workerid  + seq + trial ~ variable, data = md, value.var = "value")
dc$seq <- NULL

#make everything factors
dc$criticalComplicated <- as.factor(dc$criticalComplicated)
dc$criticalSimple   <- as.factor(dc$criticalSimple)
dc$langCondition   <- as.factor(dc$langCondition)
dc$objCondition   <- as.factor(dc$objCondition)
dc$response   <- as.factor(dc$response)
dc$responseSide   <- as.factor(dc$responseSide)
dc$responseValue   <- as.factor(dc$responseValue)
dc$word <- as.factor(dc$word)

# relable length to be numeric
dc$len <- 1
dc$len[dc$langCondition=='"three"'] <- 3
dc$len[dc$langCondition=='"five"'] <- 5
```

###  *plot by length condition*
```{r, warning = FALSE, message = FALSE, output = FALSE}
# get props
ms = ddply(dc ,.(len), function (d) {p.fc(d)}, .inform = TRUE)

#plot
qplot(len,p_complex, position=position_dodge(),
      data=ms,geom="line",ylab="Proportion selection complex object", xlab="Number of syllables")  +
  geom_linerange(aes(ymin=ciwl,ymax=ciul), position=position_dodge(.9)) +
  geom_point(aes(len,p_complex), position=position_dodge(.9), size = 3) +
  ylim(0,1)+
  geom_abline(slope=0,intercept=50,lty=2) +
  theme_bw() +
  theme(axis.title = element_text( face = "bold")) +
  theme(axis.text.x = element_text(colour = 'black')) +
  theme(axis.text.y = element_text( colour = 'black')) +
  theme(legend.position="none") +
  geom_abline(intercept = .5, slope = 0, linetype = 2)  #

summary(glm(responseValue ~ len, data=dc, family = "binomial"))
```

***
***

<a name="novelRealObjs"/>

## (2) Novel real objects

<a name="2a"/>

### (A) Norms [Complexity norming task][task30] [RT task][task9]

### (1) Complexity Norms
### *get data into long form, compute correlations between 2 samples, figure out quintiles, remove dupliate subjects*
```{r objects_complexity_norms, warning = FALSE}
if (processNorms){
  ## Sample #1
  da <- read.csv("data/RefComplex9a.results",sep="\t",header=TRUE)
  da[,31:92] <- lapply(da[,31:92],as.character)
  da[,31:92] <- lapply(da[,31:92],as.numeric)
  
  # melt
  md <- melt(da,id.vars=c("workerid"),measure.vars=names(da)[grepl("rating",names(da))])
  names(md) <- c("workerid", "rating", "value")
  ms = aggregate(value ~ rating, md, mean)

  ## Sample #2
  db <- read.csv("data/RefComplex9b.results",sep="\t",header=TRUE)
  db[,30:92] <- lapply(db[,30:92],as.character)
  db[,30:92] <- lapply(db[,30:92],as.numeric)
  
  # melt
  mdb <- melt(db,id.vars=c("workerid"),measure.vars=names(db)[grepl("rating",names(db))])
  names(mdb) <- c("workerid", "rating", "value")
  msb = aggregate(value ~ rating, mdb, mean)
  
  # merge two samples together to get lists for experiments 
  all_ratings = rbind(md, mdb)
  all_ms = aggregate(value ~ rating, all_ratings, mean)
  all_ms$ratingNum <- matrix(sapply(str_split(matrix(sapply(str_split(all_ms$rating,"rating"),
                                                            function(x) {x[2]})),"_"),function(x){x[1]}))
  all_ms$ratingNum<- as.numeric(str_replace_all(as.character(all_ms$ratingNum),"\\\"",""))
  all_ms$cil = aggregate(value ~ rating, all_ratings, ci.low)$value
  all_ms$cih = aggregate(value ~ rating, all_ratings, ci.high)$value
  
  all_ms = all_ms[!is.na(all_ms$ratingNum),] # get rid of ball and motherboard
  
  # add back in ratings for each samples
  all_ms = merge(all_ms, ms, by="rating")
  all_ms = merge(all_ms, msb, by="rating")
  
  all_ms$rating <- NULL
  names(all_ms) = c("meanRating" , "ratingNum", "cil", "cih" ,"rating_1","rating_2" )
  all_ms <- all_ms[c(2,1,3:6)]
  
  # get quintiles
  q = quantile(all_ms$meanRating, seq(0,1, by=.2))
  one = all_ms[which(all_ms$meanRating<q[2]), "ratingNum"]
  two = all_ms[which(all_ms$meanRating>q[2] & all_ms$meanRating<q[3]), "ratingNum"]
  three = all_ms[which(all_ms$meanRating>q[3] & all_ms$meanRating<q[4]), "ratingNum"]
  four = all_ms[which(all_ms$meanRating>q[4] & all_ms$meanRating<q[5]), "ratingNum"]
  five = all_ms[which(all_ms$meanRating>q[5]), "ratingNum"]
  
  one # 13 15 19 20 28 29  3 44 46 54 57 59
  two # 10 17  2 22 34 37  4 49  5 55  6  9
  three # 12 50 7 8 48 16 1 39 40 56 24 60
  four # 26 18 11 47 42 30 23 31 51 58 41 45
  five # 14 21 25 27 32 33 35 36 38 43 52 53
  
  all_ms$quintile = ifelse(is.element(all_ms$ratingNum, one), 1, 
                           ifelse(is.element(all_ms$ratingNum, two), 2,
                                  ifelse(is.element(all_ms$ratingNum, three), 3,
                                         ifelse(is.element(all_ms$ratingNum, four), 4,
                                                ifelse(is.element(all_ms$ratingNum, five), 5,
                                                       "error")))))
  all_ms$quintile = as.numeric(all_ms$quintile)
  all_ms <- all_ms[order(all_ms$meanRating),]
  
  # there is one participant who was in both samples. Look at correlation between samples without this participant in Sample #2 
  # Sample #2 
  # melt
  mdb <- melt(db[db$workerid != 'A1BQEX75BE1AYE',],id.vars=c("workerid"),measure.vars=names(db)[grepl("rating",names(db))])
  names(mdb) <- c("workerid", "rating", "value")
  msb = aggregate(value ~ rating, mdb, mean)
  
  co_norms_unique_sample = merge(ms, msb, by="rating")
  co_norms_unique_sample$ratingNum <- as.numeric(matrix(sapply(str_split(matrix(sapply(str_split(co_norms_unique_sample$rating,"rating")
                                                                                       ,function(x) {x[2]})),"_"),function(x){x[1]})))
  co_norms_unique_sample = co_norms_unique_sample[!is.na(co_norms_unique_sample$ratingNum),] # get rid of ball and motherboard

  co_norms_unique_sample$rating <- NULL
  names(co_norms_unique_sample) = c( "rating_1", "rating_2", "ratingNum")
  print(paste('reliability, removing duplicate subj:', round(cor(co_norms_unique_sample$rating_1, co_norms_unique_sample$rating_2),2))) # nearly identical to original sample
  
  # write to csv (use write.table to can exclude headers, so matlab can read for figure)
  write.table(all_ms, file="data/complicatedNormsObjs_BYITEM-m.csv", 
            row.names=FALSE, col.names=FALSE, sep=",")
  
  # write to csv
  write.csv(all_ms, "data/complicatedNormsObjs_BYITEM.csv")
  }

co_norms = read.csv("data/complicatedNormsObjs_BYITEM.csv")
```

### *get reliability between two samples*
```{r}
cor.test(co_norms$rating_1, co_norms$rating_2)
```

### (2) RT Norms
### *get data into long form (accuracy and RT df) and do exclusions*
```{r objects_rt_norms, warning = FALSE}
if (processNorms){
  raw <- read.csv("data/RefComplex30.results",sep="\t",header=TRUE)
  
  if (whichSubjRemove == 'repeatSubj') {
    raw = merge(raw, dups, by=c("hitid","workerid"))
    raw = raw[!raw$repeatSubj,]
    } else if (whichSubjRemove == 'withinRepeatSubj') {
      raw = merge(raw, dups, by=c("hitid","workerid"))
      raw = raw[!raw$withinRepeatSubj,]
      }
    
  # look at accuracy, and exclude those below chance
  boxplot(raw$Answer.correct, main = "distribution before accuracy exclusions")
  raw = raw[raw$Answer.correct > 30, ]
  
  # accuracy dataframe
  n <- names(raw)
  cols = c( n[grepl("test",n)])
  mda <- melt(raw,id.vars=c("workerid"), measure.vars=cols,na.rm=TRUE)
  mda$trial <-as.numeric(matrix(lapply(str_split(mda$variable,"_"),function(x) {x[3]})))
  mda$var1 <- matrix(lapply(str_split(mda$variable,"_"),function(x) {x[1]}))
  mda$var2 <- matrix(lapply(str_split(mda$variable,"_"),function(x) {x[2]}))
  mda$var <- paste(mda$var1, mda$var2, sep = "_")
  mda$variable <- NULL; mda$var1 <- NULL; mda$var2 <- NULL
  
  mda$seq <- with(mda, ave(value, workerid, var, FUN = seq_along))
  da = dcast(workerid + seq + trial ~ var, data = mda, value.var = "value")
  da$seq <- NULL
  
  da=da[!is.na(da$trial),]
  
  da$Answer.test_answer = as.factor(da$Answer.test_answer)
  da$Answer.test_answerEval = as.factor(da$Answer.test_answerEval)
  da$Answer.test_image  = as.factor(da$Answer.test_image)
  
  # look at memory performance (correct: CR or hit)
  numCR = length(which(da$Answer.test_answerEval == "\"CR\""))
  numH = length(which(da$Answer.test_answerEval == "\"H\""))

  correct = (numCR + numH)/ dim(da)[1]
  print(paste("percent correct:" , round(correct,2)))
  
  # RT dataframe
  n <- names(raw)
  cols = c(  n[grepl("train",n)], n["correct"])
  cols = cols[1:154]
  md <- melt(raw,id.vars=c("workerid"), measure.vars=cols,na.rm=TRUE)
  md$trial <-as.numeric(matrix(lapply(str_split(md$variable,"_"),function(x) {x[3]})))
  md$var1 <- matrix(lapply(str_split(md$variable,"_"),function(x) {x[1]}))
  md$var2 <- matrix(lapply(str_split(md$variable,"_"),function(x) {x[2]}))
  md$var <- paste(md$var1, md$var2, sep = "_")
  md$variable <- NULL; md$var1 <- NULL; md$var2 <- NULL
  
  md$seq <- with(md, ave(value, workerid, var, FUN = seq_along))
  d = dcast(workerid + seq + trial ~ var, data = md, value.var = "value")
  d$Answer.train_NA <- NULL; d$seq <- NULL
  
  d=d[!is.na(d$trial),]
  d=d[!is.na(d$Answer.train_image),]
  d=d[!is.na(d$Answer.train_rt),]
  d=d[d$Answer.train_image != "undefined",]
  d=d[d$Answer.train_image != "",]
  
  d$Answer.train_image = as.factor(d$Answer.train_image)
  d$Answer.train_rt = as.numeric(d$Answer.train_rt)
  
  #exclude outlier 2 standard deviations above and below mean (in log space)
  d$log.rt = log(d$Answer.train_rt)
  total = dim(d)[1]
  sd2 = 2*sd(d$log.rt)
  d = d[(d$log.rt > (mean(d$log.rt) - sd2)) & (d$log.rt < (mean(d$log.rt) + sd2)),]
  print(paste("percent trimmed:" , round((total - dim(d)[1])/total,2)))
  hist(d$log.rt)
  
  # aggregate
  ms <- aggregate(log.rt  ~ Answer.train_image, data=d, mean)
  ms$rt_cil <- aggregate(log.rt ~ Answer.train_image, data=d, ci.low)$log.rt 
  ms$rt_cih <- aggregate(log.rt ~ Answer.train_image, data=d, ci.high)$log.rt 
  
  ms = ms[ms$Answer.train_image != 61,] # get rid of ball and motherboard
  ms = ms[ms$Answer.train_image != 62,] 

  write.csv(ms,"data/rtNormsObjs_BYITEM.csv")
  }

rto_norms = read.csv("data/rtNormsObjs_BYITEM.csv")
```

### *get correlation between between complexity rating and RT*
```{r}
index <- match(co_norms$ratingNum, rto_norms$Answer.train_image)
co_norms$log.rt <- rto_norms$log.rt[index]
cor.test(co_norms$meanRating, co_norms$log.rt)
```

<a name="2b"/>

### (B) Mapping task [(Task)][task34]

### *get data into long form*
```{r objects_mapping, warning = FALSE}
if (doSlow) {
  d <- read.csv("data/RefComplex35.results",sep="\t",header=TRUE)
  
  if (whichSubjRemove == 'removeRepeatSubj') {
    d = merge(d, dups, by=c("hitid","workerid"))
    d = d[!d$repeatSubj,]
    } else if (whichSubjRemove == 'withinRepeatSubj') {
      d = merge(d, dups, by=c("hitid","workerid"))
      d = d[!d$withinRepeatSubj,]
      }
  
  # get in long form
  # get trial info
  md <- melt(d,id.vars=c("workerid"),
             measure.vars=c(names(d)[c(grepl("_",names(d)))]))
  md$trial <- matrix(lapply(str_split(md$variable,"_"),function(x) {x[2]}))
  md$variable <- as.character(matrix(lapply(str_split(md$variable,"_"),function(x) {x[1]})))
  md$variable <- matrix(lapply(str_split(md$variable,"Answer."),function(x) {x[2]}))
  
  md$variable <- as.factor(as.character(md$variable))
  md$trial <- as.factor(as.character(md$trial))
  md$value <- as.factor(as.character(md$value))
  md$workerid <- as.factor(as.character(md$workerid))
  
  md$seq <- with(md, ave(value, workerid,  variable, trial, FUN = seq_along))
  dc = dcast(workerid + seq + trial ~ variable, data = md, value.var = "value")
  dc$seq <- NULL
  
  write.csv(dc, "data/RC35_long.csv")
  }

dc <-  read.csv("data/RC35_long.csv")
```

### *make everything factors*
```{r}
dc$criticalComplicated= gsub(" ", "", gsub("[[:punct:]]", "", dc$criticalComplicated))
dc$criticalSimple= gsub(" ", "", gsub("[[:punct:]]", "", dc$criticalSimple))

dc$criticalComplicated <- as.factor(dc$criticalComplicated)
dc$criticalSimple <- as.factor(dc$criticalSimple)
dc$langCondition <- as.factor(dc$langCondition)
dc$objCondition <- as.factor(dc$objCondition)
dc$response <- as.factor(dc$response)
dc$responseSide <- as.factor(dc$responseSide)
dc$responseValue<- as.factor(dc$responseValue)
dc$word <- as.factor(dc$word)
dc$trial <- as.numeric(dc$trial)
```

### *merge in norms*
```{r}
index <- match(dc$criticalSimple, co_norms$ratingNum)
dc$criticalSimple_c.norms <- co_norms$meanRating[index]
index <- match(dc$criticalComplicated, co_norms$ratingNum)
dc$criticalComplicated_c.norms <- co_norms$meanRating[index]

index <- match(dc$criticalSimple, rto_norms$Answer.train_image)
dc$criticalSimple_rt.norms <- rto_norms$log.rt [index]
index <- match(dc$criticalComplicated, rto_norms$Answer.train_image)
dc$criticalComplicated_rt.norms <- rto_norms$log.rt [index]

dc$c.ratio = dc$criticalSimple_c.norms/dc$criticalComplicated_c.norms
dc$rt.ratio = dc$criticalSimple_rt.norms/dc$criticalComplicated_rt.norms
```

### *get effect sizes*
```{r}
de <- ddply(dc, .(objCondition), function (d) {d.fc(d)})
```

### *get obj conditions*
```{r}
de$cond1 <- as.factor(unlist(matrix(lapply(str_split(de$objCondition ,"-"),function(x) {x[1]}))))
de$cond2 <- as.factor(unlist(matrix(lapply(str_split(de$objCondition ,"-"),function(x) {x[2]}))))
de$cond1<- as.factor(gsub("[[:punct:]]", "", de$cond1))
de$cond2<- as.factor(gsub("[[:punct:]]", "", de$cond2))
de$objRatio = as.numeric(de$cond1)/as.numeric(de$cond2)
de$l.objRatio <- log(de$objRatio)

de$objCondition2 = paste(de$cond1, "/", de$cond2, sep = "")
```

### *plot complexity ratios vs. condition*
```{r}
# set graphical params
fs = 15
rs = 8

# complexity plot
obj_c_plot = ggplot(de, aes(y=effect_size, x=c.Mratio)) +
  geom_pointrange(aes(ymax = cill, ymin=ciul),position="dodge")+
  geom_hline(yintercept=0,lty=2) +
  stat_smooth(method="lm") +
  ylab("Effect Size (Cohen's d)") +
  xlab("Complexity Rating Ratio") + 
  #scale_x_continuous(limits = c(.25, 1.29)) +
  theme(text = element_text(size=fs), plot.title = element_text(size=20)) +
  annotate("text", x=1, y=.5, col = "red",label=paste(
    "r=",round(cor(de$effect_size, de$c.Mratio, use = "complete"), 2)), size = rs) +
  theme(plot.background = element_blank(),
  panel.grid.major = element_blank(),
  panel.grid.minor = element_blank(),
  panel.border = element_blank(),
  axis.line = element_line(color = 'black')) +
  scale_y_continuous(limits = c(-.33, .66)) 

# RT ratio plot
obj_rt_plot = ggplot(de, aes(y=effect_size, x=rt.Mratio)) +
  geom_pointrange( aes(ymax = cill, ymin=ciul))+
  geom_hline(yintercept=0,lty=2) +
  stat_smooth(method="lm") +
   ylab("Effect Size (Cohen's d)") +
  xlab("Reaction Time Ratio") +
  #scale_x_continuous(limits = c(.949, 1.005)) +
  theme(text = element_text(size=fs), plot.title = element_text(size=20)) +
  annotate("text", x=1, y=.5, col = "red",label=paste("r=",round(cor(de$effect_size, de$rt.Mratio, use = "complete"), 2)), size = rs) +  
  theme(plot.background = element_blank(),
  panel.grid.major = element_blank(),
  panel.grid.minor = element_blank(),
  panel.border = element_blank(),
  axis.line = element_line(color = 'black')) +
  scale_y_continuous(limits = c(-.33, .66))

if (savePlots) {
  #png("figure/realobjs.png", height = 6, width = 12, res=500)
  setEPS()
  postscript("figure/realobjs.eps")
  m = multiplot(obj_c_plot, obj_rt_plot, cols = 2)
  dev.off()
  #dev.copy(png,"myfile.png",width=8,height=6,units="in",res=2000)
  #dev.off()
} else {
  multiplot(obj_c_plot, cols = 1)
  multiplot(obj_rt_plot, cols = 1)
}
```

### *correlations between effect size and complexity conditions*
```{r}
cor.test(de$c.Mratio, de$effect_size)
cor.test(de$rt.Mratio, de$effect_size)
```

<a name="2c"/>

### (\(C\)) Mapping task control (random syllables)[(Task)][task41]
### *read data and get bets as numeric*
```{r, objects_random_syllables, warning = FALSE, message = FALSE, output = FALSE}
d <- read.csv("data/RefComplex41.results",sep="\t",header=TRUE)

if (whichSubjRemove == 'removeRepeatSubj') { # FIX THIS 
  d = merge(d, dups, by=c("hitid","workerid"))
  d = d[!d$repeatSubj,]
  } else if (whichSubjRemove == 'withinRepeatSubj') {
    d = merge(d, dups, by=c("hitid","workerid"))
    d = d[!d$withinRepeatSubj,]
    }
```

### *get in long form and make everything factors*
```{r, warning = FALSE, message = FALSE, output = FALSE}
#get trial info
md <- melt(d,id.vars=c("workerid"),measure.vars=c(names(d)[c(grepl("_",names(d)))]))
md$trial <- matrix(lapply(str_split(md$variable,"_"),function(x) {x[2]}))
md$variable <- as.character(matrix(lapply(str_split(md$variable,"_"),function(x) {x[1]})))
md$variable <- matrix(lapply(str_split(md$variable,"Answer."),function(x) {x[2]}))

md$variable <- as.factor(as.character(md$variable))
md$trial <- as.factor(as.character(md$trial))
md$value <- as.factor(as.character(md$value))
md$workerid <- as.factor(as.character(md$workerid))

md$seq <- with(md, ave(value, workerid,  variable, trial, FUN = seq_along))
dc = dcast(workerid  + seq + trial ~ variable, data = md, value.var = "value")
dc$seq <- NULL

#make everything factors
dc$criticalComplicated <- as.factor(dc$criticalComplicated)
dc$criticalSimple   <- as.factor(dc$criticalSimple)
dc$langCondition   <- as.factor(dc$langCondition)
dc$objCondition   <- as.factor(dc$objCondition)
dc$response   <- as.factor(dc$response)
dc$responseSide   <- as.factor(dc$responseSide)
dc$responseValue   <- as.factor(dc$responseValue)
dc$word <- as.factor(dc$word)

# relable length to be numeric
dc$len <- 1
dc$len[dc$langCondition=='"three"'] <- 3
dc$len[dc$langCondition=='"five"'] <- 5
```

### *plot by length condition*
```{r, warning = FALSE, message = FALSE, output = FALSE}
# get props
ms = ddply(dc ,.(len), function (d) {p.fc(d)}, .inform = TRUE)

#plot
qplot(len,p_complex, position=position_dodge(),
      data=ms,geom="line",ylab="Proportion selection complex object", xlab="Number of syllables")  +
  geom_linerange(aes(ymin=ciwl,ymax=ciul), position=position_dodge(.9)) +
  geom_point(aes(len,p_complex), position=position_dodge(.9), size = 3) +
  ylim(0,1)+
  geom_abline(slope=0,intercept=50,lty=2) +
  theme_bw() +
  theme(axis.title = element_text( face = "bold")) +
  theme(axis.text.x = element_text(colour = 'black')) +
  theme(axis.text.y = element_text( colour = 'black')) +
  theme(legend.position="none") +
  geom_abline(intercept = .5, slope = 0, linetype = 2)  #

summary(glm(responseValue ~ len, data=dc, family = "binomial"))
```


<a name="2d"/>
### (\(D\)) Production task [(Task)][task27]
### *get data into long form*
```{r objects_production_labels, warning = FALSE, message = FALSE, output = FALSE}
d <- read.csv("data/RefComplex27.results",sep="\t",header=TRUE)

if (whichSubjRemove == 'repeatSubj') {
    d = merge(d, dups, by=c("hitid","workerid"))
    d = d[!d$repeatSubj,]
    } else if (whichSubjRemove == 'withinRepeatSubj') {
      d = merge(d, dups, by=c("hitid","workerid"))
      d = d[!d$withinRepeatSubj,]
      }
  
n <- names(d)
d$Answer.pic_1 = as.factor(as.character(d$Answer.pic_1))
d$Answer.pic_2 = as.factor(as.character(d$Answer.pic_2))
d$Answer.pic_3 = as.factor(as.character(d$Answer.pic_3))
d$Answer.pic_4 = as.factor(as.character(d$Answer.pic_4))
d$Answer.pic_5 = as.factor(as.character(d$Answer.pic_5))
d$Answer.pic_6 = as.factor(as.character(d$Answer.pic_6))
d$Answer.pic_7 = as.factor(as.character(d$Answer.pic_7))
d$Answer.pic_8 = as.factor(as.character(d$Answer.pic_8))
d$Answer.pic_9 = as.factor(as.character(d$Answer.pic_9))
d$Answer.pic_10 = as.factor(as.character(d$Answer.pic_10))

cols = c( n[grepl("cond",n)], n[grepl("pic",n)], n[grepl("descLength",n)] )
md1 <- melt(d,id.vars=c("workerid"), measure.vars=cols,na.rm=TRUE)
md1$trial <-as.numeric(matrix(lapply(str_split(md1$variable,"_"),function(x) {x[2]})))

md1 <- melt(d,id.vars=c("workerid"), measure.vars=n[grepl("pic",n)],na.rm=TRUE)
md1$trial <-as.numeric(matrix(lapply(str_split(md1$variable,"_"),function(x) {x[2]})))
names(md1)[3]= "picture"
md1 = md1[,-2]

md2 <- melt(d,id.vars=c("workerid"), measure.vars=n[grepl("desc_",n)],na.rm=TRUE)
md2$trial <-as.numeric(matrix(lapply(str_split(md2$variable,"_"),function(x) {x[2]})))
names(md2)[3]= "description"
md2 = md2[,-2]

md3 <- melt(d,id.vars=c("workerid"), measure.vars=n[grepl("descLength",n)],na.rm=TRUE)
md3$trial <-as.numeric(matrix(lapply(str_split(md3$variable,"_"),function(x) {x[2]})))
names(md3)[3] = "length"
md3 = md3[,-2]

md4 <- melt(d,id.vars=c("workerid"), measure.vars=n[grepl("cond",n)],na.rm=TRUE)
md4$trial <-as.numeric(matrix(lapply(str_split(md4$variable,"_"),function(x) {x[2]})))
names(md4)[3] = "condition"
md4 = md4[,-2]

md12<- join(md1, md2,type = "inner")
md123<- join(md12, md3,type = "inner")
md<- join(md123, md4,type = "inner")

# add columns
md$numWords = sapply(gregexpr("\\W+", md$description), length) - 1
md <- md[md$numWords == 1,]  #remove multi word responses
md$log.length <- log(md$length) 
md$log.trial <- log(md$trial)
```

### *relationship between condition and label length*
```{r}
t.test(md[md$condition == '"complex"',"log.length"],md[md$condition == '"simple"',"log.length"],paired = TRUE)
summary(lmer(log.length~condition + (1+trial|workerid), md))
```

### *relationship between complicated norms and label length*
```{r}
index <- match(md$picture, co_norms$ratingNum)
md$c.norms <- co_norms$meanRating[index]

ms <- aggregate(log.length ~ c.norms + picture, data=md, mean)
ms$cih <- aggregate(log.length ~ c.norms + picture, data=md, ci.high)$log.length
ms$cil <- aggregate(log.length ~ c.norms + picture, data=md, ci.low)$log.length

#plot
ggplot(ms, aes(c.norms,log.length)) +
  geom_point() + 
  geom_smooth(method = "lm", color="blue", formula = y ~ x) +
  geom_errorbar(aes(ymax=log.length+cih,ymin=log.length-cil), size=0.2, colour="grey") +
  theme_bw() +
  xlab("Object Complexity Norms") +
  ylab("Log Word Length (characters)") +
  theme(axis.title=element_text(size=20), axis.text=element_text(size=15)) +
  annotate("text", x=.75, y=1.6, color = "red", size = 8,
    label=paste("r=",round(cor(md$log.length,md$c.norms), 2))) +
    ggtitle('Label length vs. complicated norms')

summary(lmer(scale(log.length)~scale(c.norms) + (1+trial|workerid), md))
```

### *relationship between RT norms and label length*
```{r}
index <- match(md$picture, rto_norms$Answer.train_image)
md$rt.norms <- rto_norms$log.rt[index]

ms <- aggregate(log.length ~ rt.norms + picture, data=md, mean)
ms$cih <- aggregate(log.length ~ rt.norms + picture, data=md, ci.high)$log.length
ms$cil <- aggregate(log.length ~ rt.norms + picture, data=md, ci.low)$log.length

#plot
ggplot(ms, aes(rt.norms,log.length)) +
  geom_point() + 
  geom_smooth(method = "lm", color="blue", formula = y ~ x) +
  geom_errorbar(aes(ymax=log.length+cih,ymin=log.length-cil), size=0.2, colour="grey") +
  theme_bw() +
  xlab("Object RT Norms") +
  ylab("Log Word Length (characters)") +
  theme(axis.title=element_text(size=20), axis.text=element_text(size=15))+
  annotate("text", x=7.5, y=1.6, color = "red", size = 8,
    label=paste("r=",round(cor(md$log.length,md$rt.norms), 2)))+
    ggtitle('Label length vs. RT norms')

summary(lmer(scale(log.length)~scale(rt.norms) + (1+trial|workerid), md))
```

***
***

<a name="xling"/>

## (3) Natural language 

<a name="3a"/>

###  (A) English Norms [(Complexity norms task)][task26]

### *preprocess and merge in stuff*
```{r english_norms, warning= FALSE}
if (processNorms) {
  d1 <- read.csv("data/RefComplex26.results1",sep="\t",header=TRUE)
  d2 <- read.csv("data/RefComplex26.results",sep="\t",header=TRUE)
  d <- rbind(d1,d2)
  
  if (whichSubjRemove == 'repeatSubj') {
    d = merge(d, dups, by=c("hitid","workerid"))
    d = d[!d$repeatSubj,]
  }
  
  # take out people who missed check question
  d = d[d$Answer.value_18 == 7, ] 
  
  # melt into word and values into two data frames, then rejoin based on workerid+trial id
  # (tricky because variable is string and number)
  n <- names(d)
  colsV =  n[grepl("value",n)] 
  colsW =  n[grepl("word_",n)] 
  mdW <- melt(d,id.vars=c("workerid"), measure.vars=colsW,na.rm=TRUE)
  mdW$trial <- as.numeric(matrix(lapply(str_split(mdW$variable,"_"),function(x) {x[2]})))
  mdW$index <- paste(mdW$workerid, mdW$trial, sep="_")
  mdV <- melt(d,id.vars=c("workerid"), measure.vars=colsV,na.rm=TRUE)
  mdV$trial <- as.numeric(matrix(lapply(str_split(mdV$variable,"_"),function(x) {x[2]})))
  mdV$index <- paste(mdV$workerid, mdV$trial, sep="_")
  
  # merge together
  index <- match(mdW$index, mdV$index)
  mdW$complexity <- mdV$value[index]
 
  # delete variables
  mdW$index <- NULL; mdW$variable <- NULL; names(mdW)[2] <- "word"; md <- mdW
  
  # remove quotes from words
  md$word= gsub(" ", "", gsub("[[:punct:]]", "", md$word))
  md$word = as.factor(md$word)
  
  # remove non-words
  md = md[md$word != "ball",] # anchor
  md = md[md$word != "motherboard",] # anchor
  md = md[md$word != "43",] # take out check question
  md = md[md$word != "peso",] # take out bad word
  
  # add length in characters
  md$nchars = nchar(as.character(md$word))
  
  # merge in word info
  # -- add mrc data --
  mrc = read.csv("data/MRC_corpus.csv")
  mrc = mrc[mrc$mrc.syl != "NA",]
  index <- match(md$word, mrc$word)
  md$mrc.fam <- mrc$mrc.fam[index]
  md$mrc.conc <- mrc$mrc.conc[index]
  md$mrc.imag <- mrc$mrc.imag[index]
  md$mrc.syl <- mrc$mrc.syl[index]
 
  # -- add phonemes from MRC --
  mrc = mrc[mrc$mrc.wtype != " ",]
  index <- match(md$word, mrc$word)
  md$mrc.phon <- mrc$mrc.phon[index]

  # -- add class --
  class = read.csv("data/english_class_codes.csv")
  index <- match(md$word, class$ENGLISH)
  md$class <- class$class_MLL[index]
  
  # -- add morphemes --
  morph = read.csv("data/numMorph_celex2.csv")
  index <- match(md$word, morph$ENGLISH)
  md$clx.morph <- morph$clx.numMorph[index]
  
  # -- add frequency --
  freqs = read.table("data/SUBTLEXusDataBase.txt",header=TRUE)
  index <- match(md$word, freqs$Word)
  md$subt.log.freq <- freqs$Lg10WF[index]
  
  # -- add brysbaert concreteness --
  b <- read.csv("data/brysbaert_corpus.csv",header=TRUE)
  b <- b[b$Word != "",] # get rid of empty rows
  b <- b[b$Bigram == 0,]# get rid of two word lemmas
  index <- match(md$word, b$Word)
  md$b.conc <- b$Conc.M[index]
  
  write.csv(md, "data/englishComplexityNorms.csv")
 }

eng = read.csv("data/englishComplexityNorms.csv")
```

### *aggregate across words of the same length*
```{r}
ms2 <- aggregate(complexity ~ nchars, data=eng, mean)
ms2$cih <- aggregate(complexity ~ nchars,  data=eng, ci.high)$complexity
ms2$cil <- aggregate(complexity ~ nchars,  data=eng, ci.low)$complexity

ggplot(ms2, aes(complexity, nchars)) +
  geom_point() + 
  geom_smooth(method = "lm", color="blue", formula = y ~ x) +
  geom_errorbarh(aes(xmax=complexity+cih,xmin=complexity-cil), size=.15, colour="black") +
  annotate("text", x=2, y=10, size = 10, label=paste("r=",round(cor(eng$complexity, eng$nchars), 2)), col="red")+
  scale_y_continuous(limits = c(0, 15), breaks = 1:14, labels = 1:14) +
  scale_x_continuous(limits = c(0, 7), breaks = 1:7, labels = 1:7)  +
  theme(axis.title=element_text(size=20), axis.text=element_text(size=15)) +
  xlab('Complexity Rating') +
  ylab('Word Length (characters)') +
  ggtitle("Number of characters vs. complexity rating")
```

### *aggregate across participants*
```{r}
  # have to do this separately for each length variable because drops words for which there are NAs *for any of the length vars*
  ms.syl <- aggregate(complexity ~ word + mrc.syl, data=eng, mean)
  ms.phon <- aggregate(complexity ~ word + mrc.phon, data=eng, mean)
  ms.morph <- aggregate(complexity ~ word + clx.morph, data=eng, mean)
  ms.chars <- aggregate(complexity ~ word + nchars +subt.log.freq + mrc.conc, data=eng, mean)

```

### *correlations between length and complexity in English*
```{r}
# correlation between syllables and complexity
cor.test(ms.syl$complexity, ms.syl$mrc.syl)

# correlation between phonemes and complexity
cor.test(ms.phon$complexity, ms.phon$mrc.phon)

# correlation between morphomes and complexity
cor.test(ms.morph$complexity, ms.morph$clx.morph)

# mono-morphemic
ms_mono.syl = aggregate(complexity ~ word + mrc.syl, data=eng[eng$clx.morph == 1,], mean)
ms_mono.phon = aggregate(complexity ~ word + mrc.phon, data=eng[eng$clx.morph == 1,], mean)

# correlation between syllables and complexity [mono-morphemic only]
cor.test(ms_mono.syl$complexity, ms_mono.syl$mrc.syl)

# correlation between phonemes and complexity [mono-morphemic only]
cor.test(ms_mono.phon$complexity, ms_mono.phon$mrc.phon)

# open class
ms_open.syl = aggregate(complexity ~ word + mrc.syl, data=eng[eng$class != 0,], mean)
ms_open.phon = aggregate(complexity ~ word + mrc.phon, data=eng[eng$class != 0,], mean)
ms_open.morph = aggregate(complexity ~ word + clx.morph, data=eng[eng$class != 0,], mean)

# correlation between syllables and complexity [open class only]
cor.test(ms_open.syl$complexity, ms_open.syl$mrc.syl)

# correlation between phonemes and complexity [open class only]
cor.test(ms_open.phon$complexity, ms_open.phon$mrc.phon)

# correlation between morphemes and complexity [open class only]
cor.test(ms_open.morph$complexity, ms_open.morph$clx.morph)

# paritialing out frequency
pcor.test(ms.chars$complexity,ms.chars$nchars,ms.chars$subt.log.freq)

# concreteness split
ms.chars <- aggregate(complexity ~ word + nchars + mrc.conc, data=eng, mean) # do this again without freq because drops missing words
ms.chars$conc.split = ifelse(ms.chars$mrc.conc < median(ms.chars$mrc.conc), 1, 2)
ms.charsL$conc.split=as.factor(ms.charsL$conc.split)
ms.charsL = ms.chars[ms.chars$conc.split == 1,]
cor.test(ms.charsL$nchars, ms.charsL$complexity)

ms.chars <- aggregate(complexity ~ word + nchars + mrc.conc + subt.log.freq, data=eng, mean) # do this again without freq because drops missing words
ms.chars$conc.split = ifelse(ms.chars$mrc.conc < median(ms.chars$mrc.conc), 1, 2)
ms.charsL = ms.chars[ms.chars$conc.split == 1,]
pcor.test(ms.charsL$complexity,ms.charsL$nchars,ms.charsL$subt.log.freq)
```

### *are the correlations between length and complexity reliable, controlling for everything? [yes]*
```{r}
# all
m_syl = lm(mrc.syl ~ complexity + mrc.fam + mrc.imag + b.conc + subt.log.freq, eng)
m_phon = lm(mrc.phon ~ complexity + mrc.fam + mrc.imag + b.conc + subt.log.freq, eng)
m_morph = lm(clx.morph ~ complexity + mrc.fam + mrc.imag + b.conc + subt.log.freq, eng)
summary(m_syl)
summary(m_phon)
summary(m_morph)

# mono-morphemic
m_syl_m = lm(mrc.syl ~ complexity + mrc.fam + mrc.imag + b.conc + subt.log.freq, eng[eng$clx.morph == 1,])
m_phon_m = lm(mrc.phon ~ complexity + mrc.fam + mrc.imag + b.conc + subt.log.freq, eng[eng$clx.morph == 1,])
summary(m_syl_m)
summary(m_phon_m)

# open class
m_syl_o = lm(mrc.syl ~ complexity + mrc.fam + mrc.imag + b.conc + subt.log.freq, eng[eng$class != 0,])
m_phon_o = lm(mrc.phon ~ complexity + mrc.fam + mrc.imag + b.conc + subt.log.freq, eng[eng$class != 0,])
m_morph_o = lm(clx.morph ~ complexity + mrc.fam + mrc.imag + b.conc + subt.log.freq, eng[eng$class != 0,])
summary(m_syl_o)
summary(m_phon_o)
summary(m_morph_o)
```

### *create english word data frame with relevant variables for xling analysis*
```{r}
  xling.eng <- aggregate(complexity ~ word + class, data=eng, mean)

  # -- add morphemes --
  morph = read.csv("data/numMorph_celex2.csv")
  index <- match(xling.eng$word, morph$ENGLISH)
  xling.eng$clx.morph <- morph$clx.numMorph[index]

  # -- add frequency --
  freqs = read.table("data/SUBTLEXusDataBase.txt",header=TRUE)
  index <- match(xling.eng$word, freqs$Word)
  xling.eng$subt.log.freq <- freqs$Lg10WF[index]
  
  xling.eng$class = as.factor(xling.eng$class)
  xling.eng$clx.morph = as.factor(xling.eng$clx.morph)
```

<a name="3b"/>

###  (B) Xling translation accuracy
```{r xling_accuracy}
checksR = read.csv("data/translation_accuracy.csv")[1:500,]
checksR = checksR[checksR$ENGLISH != "peso",]

accuracy = colSums(checksR[,2:13], dims = 1)/ dim(checksR[,])[1]
print(paste("total accuracy:", round(mean(accuracy),2)))
```

<a name="3c"/>

###  (\(C\)) Xling correlation between complexity and length
### *read in xling data and merge with English complexity norms*
```{r xling_corrs }
xling = read.csv("data/xling_csv.csv") 
index <- match(xling$ENGLISH, xling.eng$word)

xling$complexity <- xling.eng$complexity[index]
xling$class <- xling.eng$class[index]
xling$clx.morph <- xling.eng$clx.morph[index]
xling$subt.log.freq  <- xling.eng$subt.log.freq [index]
xling$X <- NULL

xling = xling[xling$ENGLISH != "peso",]
```

### *get correlations by language*
```{r}
lens = c(which(grepl("LEN",names(xling)))) # get length column indices
xlingCORR = xling[c(lens, which(names(xling)== "subt.log.freq"), which(names(xling)== "complexity"), which(names(xling)== "clx.morph"), which(names(xling)== "class"))] 

# get correlations with for all 499 words bootstrapped CIs
if (doSlow) {
  c_l = data.frame (language = character(), lower.ci = numeric(0), corr = numeric(0), upper.ci = numeric(0))
  levels(c_l$language) = names(xlingCORR)
  complexity_i = which(names(xlingCORR)== "complexity")
  
  for (i in 1:length(lens)){
    c_l[i, 2:4] = boot.cor(xlingCORR[,i], xlingCORR[,complexity_i])
    c_l[i, "language"] = names(xlingCORR)[i]
    }
  write.csv(c_l, "data/complexity_length_corrs.csv")
}
c_l = read.csv("data/complexity_length_corrs.csv")

# now do correlations, partialling out frequency
cmat.p = partial.r(xlingCORR[,1:82],c(1:80, which(names(xlingCORR) == "complexity")), which(names(xlingCORR) == "subt.log.freq"))
cxl = as.data.frame(cmat.p[which(row.names(cmat.p) == "complexity"),1:80])
cxl$lang = row.names(cxl)
names(cxl) = c("corr", "lang")

# merge in partials
index <- match(c_l$language, cxl$lang)
c_l$p.corr<- cxl$corr[index]

# get correlations for mono only
mono_cor = data.frame (language = character(), mono.cor = numeric(0))
complexity_i = which(names(xlingCORR)== "complexity")
levels(mono_cor$language) = names(xlingCORR)

for (i in 1:length(lens)){
  mono_cor[i, "mono.cor"] = cor(xlingCORR[xlingCORR$clx.morph == 1, i], 
                                xlingCORR[xlingCORR$clx.morph == 1, complexity_i], use = "complete")
  mono_cor[i, "language"] = names(xlingCORR)[i]
}

# merge in mono
index <- match(c_l$language, mono_cor$language)
c_l$mono.cor<- mono_cor$mono.cor[index]

# get correlations for open only
open_cor = data.frame(language = character(), open.cor = numeric(0))
complexity_i = which(names(xlingCORR)== "complexity")
levels(open_cor$language) = names(xlingCORR)

for (i in 1:length(lens)){
  open_cor[i, "open.cor"] = cor(xlingCORR[xlingCORR$class != 0, i], 
                                xlingCORR[xlingCORR$class != 0, complexity_i], use = "complete")
  open_cor[i, "language"] = names(xlingCORR)[i]
}

# merge in open
index <- match(c_l$language, open_cor$language)
c_l$open.cor<- open_cor$open.cor[index]

# prep for plotting
c_l = c_l[order(c_l$corr),] # sort by correlation
c_l$language=  as.character(tolower(lapply(str_split(c_l$language,"_"),function(x) {x[1]}))) # clean up names
c_l$language <- factor(c_l$language, levels =  rev(as.character(c_l$language)))

# add check information
coded_languages = c("english", "spanish", "welsh", "vietnamese", "russian", "portuguese", "persian", "bosnian", "french", "hebrew", "italian", "korean", "polish" )
c_l$checked = as.factor(ifelse( is.element(c_l$language, coded_languages), "yes", "no"))
```

### *plot correlations by language*
```{r, fig.width = 15}

# fix one label 
levels(c_l$language)[levels(c_l$language)=="haitian.creole"] <- "haitian creole"

if (savePlots) {pdf("figure/xling.pdf", height = 6, width = 12)}

ggplot(c_l, aes(language, corr, fill = checked)) + 
  geom_bar(stat = "identity") + 
  geom_linerange(aes(ymax=upper.ci, ymin=lower.ci)) +
  geom_point(data=c_l, mapping=aes(x=language, y=p.corr), size=2, shape = 17) +
  #geom_point(data=c_l, mapping=aes(x=language, y=mono.cor), size=2, shape = 16) +
  #geom_point(data=c_l, mapping=aes(x=language, y=open.cor), size=2, shape = 15) +
  geom_hline(y=mean(c_l$corr), lty=2) +
  ylab("Pearson's r") +
  xlab("Language") + 
  theme(plot.background = element_blank(),
   panel.grid.major = element_blank(),
   panel.grid.minor = element_blank(),
   panel.border = element_blank())  +
  theme(axis.text.x= element_text(angle=90,hjust=1,vjust=0.5, size = 11)) +
  theme(axis.title = element_text(size=15)) +
  theme(axis.line = element_line(color = 'black'))+
  scale_fill_manual(values=c("pink","red")) +
  theme(legend.position="none") +
  scale_y_continuous(limits = c(-.07, .75), expand = c(0,.007)) 
if(savePlots){dev.off()}
```

### *grand mean correlation between complexity and length*
```{r}
# grand mean correlation between complexity and length
mean(c_l$corr) 

# grand mean correlation between complexity and length, mono-morphemic only
mean(c_l$mono.cor)

# grand mean correlation between complexity and length, open class only
mean(c_l$open.cor)

# grand mean correlation between complexity and length, partialing out frequency
mean(c_l$p.corr)
```

***
***

[task30]: http://langcog.stanford.edu/expts/MLL/refComplex/Experiment30/ref_complex_30.html
[task9]:  http://langcog.stanford.edu/expts/MLL/refComplex/Experiment9/ref_complex_9.html
[task26]: http://langcog.stanford.edu/expts/MLL/refComplex/Experiment26/ref_complex_26.html 
[task34]: http://langcog.stanford.edu/expts/MLL/refComplex/Experiment34/ref_complex_34.html
[task23]: http://langcog.stanford.edu/expts/MLL/refComplex/Experiment23/ref_complex_23.html
[task27]: http://langcog.stanford.edu/expts/MLL/refComplex/Experiment27/ref_complex_27.html
[task35]: http://langcog.stanford.edu/expts/MLL/refComplex/Experiment35/ref_complex_35.html
[task37]: http://langcog.stanford.edu/expts/MLL/refComplex/Experiment37/ref_complex_37.html
[task38]: http://langcog.stanford.edu/expts/MLL/refComplex/Experiment38/ref_complex_38.html
[task40]: http://langcog.stanford.edu/expts/MLL/refComplex/Experiment38/ref_complex_40.html
[task41]: http://langcog.stanford.edu/expts/MLL/refComplex/Experiment38/ref_complex_41.html


