% Use only LaTeX2e, calling the article.cls class and 12-point type.

\documentclass[12pt]{article}


% Users of the {thebibliography} environment or BibTeX should use the
% scicite.sty package, downloadable from *Science* at
% www.sciencemag.org/about/authors/prep/TeX_help/ .
% This package should properly format in-text
% reference calls and reference-list numbers.

\usepackage{scicite}

% Use times if you have the font installed; otherwise, comment out the
% following line.

\usepackage{times}
\usepackage{graphicx}
\usepackage{caption} % removes colon after empty caption


% The preamble here sets up a lot of new/revised commands and
% environments.  It's annoying, but please do *not* try to strip these
% out into a separate .sty file (which could lead to the loss of some
% information when we convert the file to other formats).  Instead, keep
% them in the preamble of your main LaTeX source file.


% The following parameters seem to provide a reasonable page setup.
\topmargin 0.0cm
\oddsidemargin 0.2cm
\textwidth 16cm 
\textheight 21cm
\footskip 1.0cm


%The next command sets up an environment for the abstract to your paper.

\newenvironment{sciabstract}{%
\begin{quote} \bf}
{\end{quote}}


% If your reference list includes text notes as well as references,
% include the following line; otherwise, comment it out.

\renewcommand\refname{References and Notes}

% The following lines set up an environment for the last note in the
% reference list, which commonly includes acknowledgments of funding,
% help, etc.  It's intended for users of BibTeX or the {thebibliography}
% environment.  Users who ae hand-coding their references at the end
% using a list environment such as {enumerate} can simply add another
% item at the end, and it will be numbered automatically.

\newcounter{lastnote}
\newenvironment{scilastnote}{%
\setcounter{lastnote}{\value{enumiv}}%
\addtocounter{lastnote}{+1}%
\begin{list}%
{\arabic{lastnote}.}
{\setlength{\leftmargin}{.22in}}
{\setlength{\labelsep}{.5em}}}
{\end{list}}


% Include your paper's title here

\title{The length of words reflects their conceptual complexity}


% Place the author information here.  Please hand-code the contact
% information and notecalls; do *not* use \footnote commands.  Let the
% author contact information appear immediately below the author names
% as shown.  We would also prefer that you don't change the type-size
% settings shown here.

\author
{Molly Lewis and Michael C. Frank\\
\\
\normalsize{Psychology Department, Stanford University,}\\
\normalsize{450 Serra Mall, Stanford, CA 94305, USA}\\
\\
\normalsize{$^\ast$To whom correspondence should be addressed; E-mail: mll@stanford.edu.}
}


% Include the date command, but leave its argument blank.

\date{}



%%%%%%%%%%%%%%%%% END OF PREAMBLE %%%%%%%%%%%%%%%%
\begin{document} 

% Double-space the manuscript.
\baselineskip24pt

% Make the title.
\maketitle 

% Place your abstract within the special {sciabstract} environment.

% \begin{sciabstract}
					

% \end{sciabstract}					

{\bf Are the forms of words systematically related to their meaning? The arbitrariness of the sign has long been a foundational part of our understanding of human language \cite{saussure,hockett1960}. Theories of communication predict another relationship between form and meaning, however: longer descriptions should convey more complex meanings \cite{horn1984,jaeger2006}. Here we show that both the lexicons of human languages and their speakers encode the relationship between linguistic and cognitive complexity. Participants mapped longer words to more complex objects in comprehension and production tasks and across a range of stimuli. In addition, explicit judgments of complexity were highly correlated with an implicit measure of study time in a memory task, suggesting that complexity is directly related to basic cognitive processes. In addition, judgements of complexity for a sample of real words correlated highly with their length across 80 languages, even controlling for frequency, familiarity, imageability, and concreteness. These results point to a general regularity in the design of lexicons and suggest the importance of cognitive constraints on language evolution \cite{christiansen2008,lieberman2007}.}


In a classic example of pragmatic reasoning \cite{horn1984}, the utterance ``Lee got the car to stop'' seems to imply an unusual state of affairs. Had the speaker wished to convey that Lee simply applied the brakes, the shorter and less exceptional ``Lee stopped the car'' would be a better description. The use of a longer utterance licenses the inference that there was some problem in stopping---perhaps the brakes failed---and that the situation is more complex. Does this same reasoning apply to the meanings of words? Is a ``tupabugorn'' more likely to be a complex, unusual object than a ``tupa''? 

\begin{figure}[t]
\begin{center}
\includegraphics[scale = .5]{figs/geons_cropped.png}
\caption{}
\end{center}
\label{fig:geons}
\end{figure}


We tested this hypothesis by asking whether speakers would be biased to interpret a long novel word as being more likely to refer to a more complex novel referent. We presented participants on Amazon Mechanical Turk ($N=750$) with a novel word of either 2 or 4 syllables and two possible alternative object referents. Possible referents were novel artificial objects whose complexity we manipulated by varying the number of parts the object contained (1 - 5 ``geons,'' \cite{biederman1987}; Fig. 1a; these judgements were highly correlated with explicit complexity judgments, $r = .93$, $p < .01$). For every unique combination of object complexities (1 vs. 2 geons, 1 vs. 3 geons, 1 vs. 4 geons, etc.), participants were asked to select which object the word named. 

Across conditions, the more complex object was more likely to be judged the referent of the longer word. For each condition (e.g., 1 vs. 2 geons), we calculated the effect size for participants' complexity bias â€“ the degree to which a choice of the more complex object was more likely for a longer, rather than a shorter, word.  Effect size was highly correlated with the ratio of object complexities: The greater the mismatch in object complexity, the more the longer word was paired with the more complex object ($r = .87$, $p < .01$, Fig. 1b).
					

\begin{figure}[t]
\begin{center}
\includegraphics[scale = .5]{figs/real_objs_cropped.png}
\caption{}
\end{center}
\label{fig:real_objs}
\end{figure}

Next we asked whether this bias extended to more naturalistic objects. We gathered a sample of real objects without canonical labels (Fig. 2a) and asked participants to rate their complexity. These judgements were highly reliable across two independent samples (N = 60 in each, $r = .93$). We then divided the objects into quintiles based on these ratings, and used them as stimuli in a mapping task identical to the one used with the artificial objects. As with the artificial objects, effect size was negatively correlated with the complexity judgment ratio between the referent alternatives ($N = 1500$; $r = .70$, $p < .01$, Fig. 2b). We also found the same bias in language production: Participants produced novel coinages for the top quartile of objects that were longer than those for the bottom quartile ($t(57) = 3.91$, $p < .001$). 
					
If complexity is related to a basic cognitive process, we should be able to measure it using an implicit task, not just via explicit ratings. In visual cognition, stimuli that contain more information require more processing time in search \cite{alvarez2004}. To test this prediction, we measured participants' reaction time to objects in a memory task. Each participant viewed half of the objects in the stimulus set, one at a time, and then made old/new judgments for the entire set. Critically, the training phase was self-paced, such that participants were allowed to study each object for as much time as they wanted. 
					
Mean study time was highly correlated with explicit complexity norms for both artificial objects ($N = 250$, $r = .89$, $p < .01$) and novel real objects ($N = 500$, $r = .54$, $p < .01$). In addition, the ratio of study times for the two objects was correlated with the bias to choose a longer label for both the artificial objects ($r = .82$, $p < .01$; Fig. 1c) and the novel real objects ($r = .71$, $p < .01$; Fig. 2c): Longer study times predicted longer labels. These findings suggest that label judgments are supported by basic cognitive processes related to the complexity or information content of a stimulus. 
					
Together, these experiments point to a complexity bias in interpreting novel labels: Words that are longer tend to be associated with meanings that are more complex, as reflected in both implicit and explicit measures. Is this bias only relevant to judgments of unfamiliar words, or does it apply to familiar labels as well? 


\begin{figure}[t]
\begin{center}
\includegraphics[scale = .55]{figs/xling.pdf}
\caption{Correlations between conceptual complexity norms and word lengths, across languages. Dark red bars indicate languages for which translations were checked by native speakers; all other bars show translations obtained via Google Translate. Error bars show 95\% confidence intervals obtained via non-parametric bootstrap. Triangles indicate correlation value partialling out English log frequency.}
\end{center}
\label{fig:real_objs}
\end{figure}

We collected ratings of the complexity of the meaning of 499 English words ($N = 250$). Longer English words had meanings that were rated as more complex in a similar explicit complexity rating procedure. Complexity judgements were positively correlated with word length, measured in syllables, phonemes, and morphemes ($r_{syllables} = .63$, $r_{phonemes} = .66$, $r_{morphemes} = .43$, all $p$s $< .01$), even when closed-class words were excluded ($N = 453$; $r_{syllables} = .62$, $r_{phonemes} = .66$, $r_{morphemes} = .43$, all $p$s $< .01$). Importantly, these relationships also remained reliable after controlling for the word's concreteness, imageability, and familiarity. 
						
If the complexity bias relies on a universal cognitive process, it should generalize to lexicons beyond English. We explored this prediction in 79 additional languages, using Google Translate to translate our word set. Native speakers checked the accuracy of these translations for 12 of the 79 languages, finding an accuracy of .92 within this sample. For each language, we calculated the correlation between word length in terms of number of characters (to allow comparison between languages for which no phonetic dictionary was available) and mean complexity rating. All 79 languages showed a positive correlation between length and complexity ratings (Fig. 3). The grand mean correlation across languages was .34. 
					
Word length is also strongly related to linguistic predictability, operationalized via simple frequency \cite{zipf1936} or using a language model \cite{piantadosi2011a}. But the regularity we describe---a relationship between conceptual complexity and word length---holds even when controlling for frequency. In English, the correlation was only slightly reduced when controlling for log frequency ($r = XYZ$, $p < .001$).  Across languages, partialling out log frequency (estimated in English), the grand mean correlation was .22. In addition, in a number of experiments, when we manipulated the observed frequencies of novel objects, we found no effects on judgments of word length (see SI). 

Languages also show phonological iconicity effects, such that semantic features \cite{marr} and even particular form classes \cite{farmer2006,} are marked by particular sound patterns. Our findings could be due to some broad iconic relationship between abstract measures of complexity and amount of verbal or orthographic effort---indeed this is precisely the relationship that Horn originally noticed. But specific iconic hypotheses that posit a parallel between an object's parts and the number of phonemes, morphemes, or syllables in its label do not account for the patterns in the English lexicon, which hold for monomorphemic words alone ($N = 387$; $r_{syllables} = .46$, $r_{phonemes} = .53$, all $p$s $< .01$) and for abstract ideas that have no obvious part structure (concreteness split?).

CONCLUSION


\paragraph*{Methods}


\paragraph*{Acknowledgements}


\paragraph*{Author Contributions}


\paragraph*{Author Information}



\bibliography{biblibrary}

\bibliographystyle{Science}




% Following is a new environment, {scilastnote}, that's defined in the
% preamble and that allows authors to add a reference at the end of the
% list that's not signaled in the text; such references are used in
% *Science* for acknowledgments of funding, help, etc.



% For your review copy (i.e., the file you initially send in for
% evaluation), you can use the {figure} environment and the
% \includegraphics command to stream your figures into the text, placing
% all figures at the end.  For the final, revised manuscript for
% acceptance and production, however, PostScript or other graphics
% should not be streamed into your compliled file.  Instead, set
% captions as simple paragraphs (with a \noindent tag), setting them
% off from the rest of the text with a \clearpage as shown  below, and
% submit figures as separate files according to the Art Department's
% instructions.


\clearpage



\end{document}



