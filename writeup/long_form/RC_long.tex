\documentclass[man]{apa2}
\usepackage{pslatex}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{color}
\usepackage{covington}
\usepackage[usenames,dvipsnames]{xcolor}

\title{The length of words reflects their conceptual complexity}

\twoauthors{Molly L. Lewis}{Michael C. Frank}
\twoaffiliations{Department of Psychology, Stanford University}{Department of Psychology, Stanford University}


\abstract{Are the forms of words systematically related to their meaning? The arbitrariness of the sign has long been a foundational part of our understanding of human language. Theories of communication predict a relationship between length and meaning, however: Longer descriptions should be more conceptually complex. Here we show that both the lexicons of human languages and individual speakers encode the relationship between linguistic and conceptual complexity. Experimentally, participants mapped longer words to more complex objects in comprehension and production tasks and across a range of stimuli. Explicit judgments of conceptual complexity were also highly correlated with implicit measures of study time in a memory task, suggesting that complexity is directly related to basic cognitive processes. Observationally, judgements of conceptual complexity for a sample of real words correlate highly with their length across 80 languages, even controlling for frequency, familiarity, imageability, and concreteness. While word lengths are systematically related to usage---both frequency and contextual predictability---our results reveal a systematic relationship with meaning as well. They point to a general regularity in the design of lexicons and reinforce the importance of cognitive constraints on language evolution.

~\\

Keywords: xx}

\shorttitle{The length of words reflects their conceptual complexity}
\rightheader{The length of words reflects their conceptual complexity}

\acknowledgements{ We gratefully acknowledge the support of ONR Grant N00014-13-1-0287.

~\\

\noindent Address all correspondence to Molly L. Lewis, Stanford University, Department of Psychology, Jordan Hall, 450 Serra Mall (Bldg. 420), Stanford, CA, 94305. Phone: 650-721-9270. E-mail: \texttt{mll@stanford.edu}}

\begin{document}

\maketitle                            


\section{Introduction}

Human languages are systems for encoding information about the world. A defining feature of a symbolic coding system is that there is no inherent mapping between the form of the code and what the code denotes  \cite{peirce}---the color red holds no natural relationship to the meaning `stop', the numeral {\it 3} holds no natural relationship to three units, and in language, the word `horse' looks or sounds nothing like the four-legged mammal it denotes.  The arbitrariness of the linguistic sign has long been observed as a fundamental and universal property of natural language \cite{saussure, hockett1960}. And, despite the growing number of cases suggesting instances of non-arbitrariness in the lexicon \cite<for a review,>{schmidtke2014phonological}, there is clear evidence for at least some degree of arbitrariness in language based only on the observation that different languages use different words to denote the same meaning  (e.g. the word for horse in English is ``horse" but is ``at" in Turkish). 

%2 consequences - word learning and catgorical/flexibility. 

Importantly, however, the arbitrary property of language is true only from the perspective of the outside analyst. From the perspective of the language user, language is highly non-arbitrary; there are strong constraints on how language is used in particular instances of communication.  A rich body of theoretical work has explored regularities in the use of  particular forms to refer to particular  types of meanings in context---the study of {\it pragmatics} \cite{grice1975logic,horn1984, clark1996using}. Broadly, this work argues that  language users assume certain regularities in how speakers refer to meanings, and through these shared assumptions, the symmetry of the otherwise arbitrary character of language is broken. For example, consider a speaker who intends to refer to a particular apple on a table. Because language is {\it a priori} arbitrary, there are a range of  ways the speaker could convey this meaning (e.g, ``the apple," ``the banana," ``the green apple,"``the green apple next to the plate," etc.), but the speaker is constrained by pragmatic pressures of the communicative context. If the listener also speaks English, the phrase ``the banana" will be an  ineffective way to refer to the apple. Furthermore, if there is only one apple on the table, the phrase ``the green apple" will be unnecessarily verbose given the referential context. These constraints might lead a speaker to select ``the apple" as the referential phrase, because it both allows the listener to correctly identify the intended referent while also minimizing effort on the part of the speaker. 

In the present paper, we examine whether principles of communication influence the otherwise arbitrary mappings between words and meanings in the lexicon. In particular, we explore a communicative regularity first observed by \citeA{horn1984}: pragmatic language users tend to consider the effort that speakers have exerted to convey a meaning. For example, the utterance ``Lee got the car to stop" seems to imply an unusual state of affairs. Had the speaker wished to convey that Lee simply applied the brakes, the shorter and less exceptional ``Lee stopped the car" would be a better description. The use of a longer utterance licenses the inference that there was some problem in stopping---perhaps the brakes failed---and that the situation is more complex. Do we reason the same way about the meanings of words, breaking the symmetry between two unknown meanings by reference to length? In other words, is a ``tupabugorn" more likely to be a complex, unusual object than a ``ralex"? We examine whether this  regularity---a {\it complexity bias}---is present in the lexicon.

The present hypothesis is motivated by the possibility that language dynamics take place over different timescales, and these different dynamics may be causally related to each other.  Minimally, two timescales are relevant to the present hypothesis. At the shorter timescale are the minutes of a single communicative interaction---the pragmatic timescale. At the longer timescale is language change, which takes place over many years---the language evolution timescale. We consider the possibility that communicative pressures at the pragmatic timescale may, over time, influence the structure of the lexicon at the language evolution timescale. There are, however, other reasons why a regularity like a complexity bias might emerge in the structure of the lexicon, and we consider some of these alternative possibilities in the General Discussion.

The plan of the paper is as follows. We first review prior work suggesting that communicative principles are reflected in the structure of the lexicon. We then review work related to accounts of our particular linguistic feature of interest---variability in the length of forms. Next, we   present four sets of studies that explore a complexity bias in the lexicon. In Studies 1 and 2, we  experimentally test whether participants are biased to map a relatively long novel word onto a relatively more complex object, using artificial objects (Study 1) and novel, real objects (Study 2). In Study 3, we explore the underlying cognitive construct of complexity in a reaction time task. Finally, in Study 4, we examine a complexity bias in natural language through a corpus analysis of 80 languages.

%they are inherently symmetric at all levels. There is no reason to associate a particular form---whether an utterance, a word, or a phoneme---with a particular meaning. arbitrariness of the linguistic sign A 
%Natural language, however, is a special kind of coding system: it is {\it hierarchical}. Many proposals have been made about constraints that govern the types of forms that refer to proposals have been made about the However, as a communicative system for signaling information, at the level of phrase there are a number of constraints on arbitrariness communicative stuff

 %They are That is, they are used by a group of of speakers who interact with each other with the intention of sharing information. A large body of theoretical work has examined the regularities that govern what language forms are used 
% Broadly, this work suggests that, while language is entirely symmetric a priori, there are a number of communicative constraints that break the symmetry.

 %For example, if a speaker wants to refer to a horse, and says `banana,' the speaker will fail at conveying her intended meaning, despite the fact that a priori both `banana' and `horse' are equally good words for the meaning horse. 


%Importantly, however, coding systems denote meaning in two different ways. 

%natural languages have a second defining characteristic as a coding system: they are hierarchical. Languages contain units of meanings---words---that can be combined into larger units---phrases, sentences, and discourse---to flexibly express complex ideas. At the level of these larger units, a rich body of theoretical work has explored regularities in the use of particular forms to refer to particular types of meanings --- the study of {\it pragmatics} \cite{horn, grice, clark}. For example, using the phrase 'green one' instead of the `the green su' { Broadly, this work suggests that, while language is entirely symmetric at the level of words, at the level of the phrase and beyond, there are a number of communicative constraints that break the symmetry.

%*timescales communicative and hierarchical * signals

\section{Pragmatic equilibria in the lexicon}

While several broad theories of pragmatics have been proposed, most include a version of  two critical pressures on communication: the desire to minimize effort in speaking and the desire to be informative \cite{zipf1936, horn1984}. Importantly, these two pressures tradeoff with each other: the optimal solution to the first pressure is a single word that can refer to all meanings, while the optimal solution to the second pressure is a verbose, minimally ambiguous  phrase. The utterance that emerges is argued to be an equilibrium between these two tradeoffs.

At the timescale of language evolution, there a number of cases in which these pragmatic equilibria  are reflected in the lexicon. One way these equilibria are reflected is in the size of the semantic space denoted by a particular word. From the hearer's perspective, Horn argues there is a pressure  to narrow semantic space \cite{horn1984}. This reflects the idea that the hearer's optimal language is one in which every possible meaning receives its own word. One example of this is the word ``rectangle." This word refers to a quadrilateral with four right angles. A special case of a ``rectangle"  is a case where the four sides are equal in length, which has its own special name, ``square." Consequently, the term ``rectangle" has been narrowed to mean a quadrilateral with four right angles, where the four sides are {\it not} equal. From the speaker's perspective, there is a pressure for semantic broadening. This is because the speaker's ideal language is one in which a single word can refer to a wide range of meanings. An example of this is the broadening of brand names to refer to a kind of product. For example, ``kleenex" is a name of a product name for facial tissues, but has taken on the meaning of facial tissues more generally.

The opposition of these two semantic forces predicts an equilibrium in the organization of semantic space that satisfies the pressures of both speaker and hearer. A body of empirical work has tested this prediction by examining the organization of particular semantic domains cross-linguistically \cite{regierword}. Languages show a large degree of similarity in how they partition semantic space for a particular domain, which is likely due to universal cognitive constraints. But, they also show a large degree of variability and these different systems can be shown to all approximate an equilibrium point between speaker and hearer pressures. 

 \citeA{kemp2012kinship} demonstrate this systematicity in the semantic domain of kinship. For each language, they developed a metric of the degree to which  Horn's speaker and hearer pressures (in their terminology: communicative cost and complexity, respectively) are satisfied. A language that better satisfies the hearer's pressure is one that is more complex, as measured by the length of the description of the system in their representational language. A language that better satisfies the speaker's pressure is one that requires less language to describe the intended referent. To understand this, consider the word ``grandmother" in English: this word is ambiguous in English because it could refer to either the maternal or paternal mother, and so identifying one in particular is more costly in English than in a language that encodes this distinction lexically. They find that the set of attested languages is a subset of the range of possible languages, and this subset partitions the  semantic space in a way that is near the optimal tradeoff between pragmatic pressures. This type of analysis has also been done for the domains of color \cite{regier2007color}, light \cite{baddeley2009}, and numerosity \cite{xu4numeral}.
 
A second phenomenon that is predicted by these forces is cases where there are multiple meanings associated with a word from a context-independent perspective, or cases of lexical ambiguity. Lexical ambiguity is present in both open-class words like ``bat"  (a baseball instrument or a flying mammal) and closed-class quantifiers like ``some" ( ``at least one and possibly all''  or ``at least one but not all"). Lexical ambiguity is toleratedbecause the meaning is usually easily disambiguated by context. When the word ``bat" is uttered while watching a baseball game,  the mammal usage of the word is very unlikely. The presence of this type of ambiguity can be viewed as an equilibrium between the two pragmatic pressures. If the meaning of a word can be disambiguated by the referential context, then it would violate the speaker's pressure to minimize effort by keeping track of two distinct words.

Indeed, recent work by \citeA{piantadosi2011b} reveals systematicity in the presence of lexical ambiguity in language. They argue that ambiguity results from a speaker based pressure to broaden the meaning of a word to include multiple possible meanings. In particular, they suggest that this pressure should lead to a systematic relationship between the presence of ambiguity and the cost of a word. According to their argument, costly words (in terms of length, frequency, or any metric of cost) that are easily understood by context violate the speaker's principle to say no more than you must. Consequently, there should be a pressure for these meanings to get mapped on to a different, less costly word. This word may happen to already have a meaning associated with it, and so the result  is multiple meanings being mapped to a single word. For example, in the case of the word ``bat," a speaker could instead say ``baseball bat." But, because this referent is easily disambiguated in context from the mammalian meaning,  Horn's speaker principle leads to a pressure to use the shorter form. This leads to a testable prediction that shorter words should tend to be more ambiguous.  Through corpus analyses, \citeA{piantadosi2011b} find this precise  relationship between cost and ambiguity. They find a linear relationship between word length and ambiguity across English, Dutch and German: Shorter words are more likely to have multiple meanings. 

An additional case of this lexical ambiguity is found in words that have very little context-independent meaning, known as indexicals or deictics \cite{frawley2003international}. These words get their meaning from the particular referential context of the utterance, and are therefore highly ambiguous from a context-independent perspective. There are many types of indexicals that are present to varying degrees across languages. An example of a temporal indexical form is ``tomorrow." The context-independent meaning of this word is something like ``the day after the day this word is being uttered in.'' Critically, abstracted from any context, this word has little meaning; it is impossible to interpret without having knowledge about the day the word was uttered. This phenomenon is also present in person pronouns (e.g.\ ``you" and ``I") and spatial forms, like ``here" and ``there."  As for lexical ambiguity, this type of ambiguity is a predicted equilibrium point from Horn's principles: If the hearer can recover the intended referent from context, the speaker would be saying more than is necessary by using an overly-specific referential term (e.g., ``December 18th, 2014" vs.``tomorrow"). Language structure reflects this pressure through lexicalized ambiguity in the form of indexicals.

Finally, the relationship between the meanings of different words can be seen as a consequence of pragmatic principles. A number of theorists have noted a bias against two words mapping onto the same meaning --- that is, a bias against synonymy \cite{saussure, kiparsky1983word, horn1984, clark1987principle, clark1988logic}. This bias is an equilibrium between Horn's speaker and hearer principles. Recall that the optimal language for a hearer is one in which each meaning maps to its own word --- exactly a language biased against synonymy. It turns out that the speaker's pressure also biases against synonymy.  The optimal language for the speaker is a language where a single word maps to all meanings. But, a case where multiple words map to a single meaning is also undesirable because the speaker must keep track of two words. So, for both the speaker and the hearer, there is pressure to avoid synonymy. Thus, when a listener hears a speaker use a second word for an existing meaning, the hearer infers that this could not be what the speaker intended because this would violate the speaker's principle. The result is an assumption that the second word maps to a different meaning. This pattern is reflected in language structure by a one-to-one pattern in the lexicon --- that is, a structure in which each word maps to exactly one meaning and each meaning maps to exactly one word.

As one kind of evidence for this one-to-one structure in the lexicon, \citeA{horn1984} points to a phenomenon called {\it blocking}. Blocking refers to cases in which an existing lexical form blocks the presence of a different, derived form with the same root. Consider the following examples:
 \begin{quote} 
 	(a) fury furious *furiosity\\
	(b) *cury curious curiosity 
\end{quote}
In both (a) and (b), forms that would be expected, given the inflectional morphology in English, are not permitted. This is presumably because they would have the same meaning as the existing form because they have the same root. Examples such as this provide some evidence for a one-to-one structure in language, but a one-to-one structure is a particularly difficult linguistic regularity to test empirically. Nonetheless, it is an important regularity because it licenses certain inferences in interpreting the meaning of words. In particular, the cognitive representation of a one-to-one regularity has been posited as an explanation of children's bias to map a novel word onto a novel object \cite{markman1988, markman2003}.

Together these phenomena---semantic organization, ambiguity, and one-to-one structure---provide three cases in which equilibria  that are predicted by theories of communication at the pragmatic timescale are reflected in the structure of the lexicon at the language evolution timescale. While this commonality does not entail causality, it is suggestive of a causal relationship between the two timescales.

\section{Accounts of language length variability}

Language forms vary along many dimensions, but a salient dimension is length: words and entire utterances can vary dramatically in their phonological length. At the pragmatic timescale, 
Horn
UID
bergen - experimental evidence

At the language change timescale, a number of 
Zipf - frequency
piantadosi - predictability
Mahowald -follow up
markedness
 
Fezchician - in the grammar






Horn's principles make a prediction about the relationship between the length of utterances and their meanings. In many cases, it is possible to use two different utterances to refer to the same meaning (in truth functional terms), and often these utterances differ in length. \citeA{horn1984} presents the following example: 
\begin{quote} 
 	(1a) Lee stopped the car.\\
	(1b) Lee got the car to stop.
	
	%(2a) Black Bart killed the sherif.\\
	%(2b) Black Bart caused the sherif to die.
\end{quote}
Both (a) and (b) have the same denotational meaning (the successful stopping of a car), but they differ in length ((b) has two extra words). Horn argues that this asymmetry leads to an inference on the part of the listener that the two differ in meaning. The logic of this inference is  identical to the lexical structure case above. The listener hears a speaker use a more costly phrase to express a meaning that could have been expressed in a less costly way. The listener thus infers that this other meaning could not be what the speaker intended because this would violate the speaker's principle to say no more than is necessary. Horn adds an additional layer to this argument. He suggests that no only do these two forms differ in meaning, but that they map onto meanings in a systematic way. In particular, he argues that the longer form gets mapped on to the more marked meaning, while the shorter form refers to the unmarked meaning.  The notion of `markedness' is underspecified here, but an intuitive definition is related to complexity: more marked things are more conceptually complex, while less marked things are more conceptual simple.  Thus, in the above example, (a) would refer to a simple, average case of car stopping, while (b) might refer to case where something complex or unusual happened, perhaps because Lee used the emergency brake.


The source of the particular mapping between forms of different lengths and meanings of different degrees of markedness is unclear. This is because, in principle, there are multiple equilibrium points in the mapping between form and meaning. Assuming a one-to-one constraint on the mapping, there are two possible equilibria: \{short--simple, long--complex\} or \{short--complex, long--simple\} (Table 4). Both satisfy the constraint that each  form gets mapped to a unique meaning. So how do speakers arrive at the  \{short--simple, long--complex\} equilibrium? This is a difficult result to derive from models of pragmatic reasoning. \citeA{bergen2014} successfully derive this result as a consequence of the fact that \{short--simple, long--complex\} is a more optimal mapping for the speaker (the indirect result of Zipf's Principle of Least Effort). Another possibility relies on iconicity: hearers have a cognitive bias to map more complex sounding forms to meanings that are similarly complex. 

Despite the absence of clear theoretical account of this phenomenon, the empirical data suggest that learners do indeed arrive at the predicted equilibrium. \citeA{bergen2012} provide evidence for this type of implicature in a communication game.  In their task, partners were told that they were in an alien world with three objects and three possible utterances of different monetary costs. They operationalize the idea of markedness or complexity as frequency, such that participants were instructed that each of the three different objects had three different base rate frequencies  associated with them.  Participants' task was to communicate about one of the objects using one of the available utterances. If they successfully communicated, they received a reward. The results suggest that both the speaker and hearer expected costlier forms to refer to less frequent meanings. This study provides one data point suggesting that Horn's predicted equilibrium between word length and meaning emerges in coordination games.  

There is a growing body of evidence suggesting this equilibrium is also reflected in the structure of words. One approach to testing this hypothesis is to use the linguistic context of a word to measure the complexity of meaning. The idea is that words that are highly predictable, given the linguistic context, have more complex meanings, while words that are less predictable given the linguistic context, have less complex meanings. \citeA{piantadosi2011a} measured  the relationship between the predictability of words in context and the length of words. Across 10 languages, these two measures were highly correlated: words that were longer were less predictable in their linguistic context on average. This result held true even controlling for the frequency of words. Additional evidence for this relationship comes from examining pairs of words that  have  very similar meaning, but differ in length \cite<e.g. ``exam" vs.\ ``examination;">{mahowald2012info}. Through corpus analyses, they find that the longer forms are used in less predicable linguistic contexts. In a behavioral experiment, they also find that  speakers are more likely to select the longer word in unsupportive contexts. This body of work points to a systematic relationship between word length and meaning, when complexity is operationalized as predictability in the linguistic context.

Some of our own work provides more direct evidence for this equilibrium. Given a novel word, we find that both adults and preschoolers are more likely to map a longer word to a more complex object, as compared to a short word \cite{lewis2014structure}. A key difference between our work from prior work is that we  directly manipulate the complexity of word meaning, rather than using the predictability of linguistic context as a proxy. We have operationalized complexity in three different ways. The first is to directly manipulate the number of object parts the referent has. Second, we have measured complexity by obtaining complexity norms from participants on real objects. Third, we have operationalized complexity through a reaction time measure. In each case, we see a bias to map longer words to more complex referents, as compared to a short word. We also find this bias in natural language. We asked participants to rate the complexity of the meaning  of 499 English words, and found that these ratings were highly correlated with word length in both English and 79 other languages. Taken together, this work provides strong evidence that the equilibrium between word length and complexity of meaning found in coordination games, such as \citeA{bergen2012}, is also reflected in the structure of the lexicon. 


Horn
UID

* frequency
* piantadosi
* markedness
* fedechica



\section{Study 1: Complexity bias with artificial objects} 
 artificial objects
\subsection{\textbf{Experiment 1a}}
norms
\subsection{Methods}
\subsubsection{Procedure}
\subsubsection{Participants}
\subsubsection{Stimuli}
\subsubsection{Procedure}

\subsection{Results}
\subsection{\textbf{Experiment 1b}}
mappiny

\subsection{Methods}
\subsubsection{Procedure}
\subsubsection{Participants}
\subsubsection{Stimuli}
\subsubsection{Procedure}

\subsection{Results}

\section{Study 2: Complexity bias with novel, real objects} 
real objects
\subsection{\textbf{Experiment 2a}}
norms
\subsection{Methods}
\subsubsection{Procedure}
\subsubsection{Participants}
\subsubsection{Stimuli}
\subsubsection{Procedure}
\subsection{Results}


\subsection{\textbf{Experiment 2b}}
mapping
\subsection{Methods}
\subsubsection{Procedure}
\subsubsection{Participants}
\subsubsection{Stimuli}
\subsubsection{Procedure}
\subsection{Results}


\subsection{\textbf{Experiment 2c}}
 replication with randomly concatenate syllables
\subsection{Methods}
\subsubsection{Procedure}
\subsubsection{Participants}
\subsubsection{Stimuli}
\subsubsection{Procedure}
\subsection{Results}


\subsection{\textbf{Experiment 2d}}
production
\subsection{Methods}
\subsubsection{Procedure}
\subsubsection{Participants}
\subsubsection{Stimuli}
\subsubsection{Procedure}
\subsection{Results}






\section{Study 3: Complexity as a cognitive construct}

\subsection{\textbf{Experiment 3a}}
artificial object
\subsection{Methods}
\subsubsection{Procedure}
\subsubsection{Participants}
\subsubsection{Stimuli}
\subsubsection{Procedure}
\subsection{Results}

\subsection{\textbf{Experiment 3b}}
real objects
\subsection{Methods}
\subsubsection{Procedure}
\subsubsection{Participants}
\subsubsection{Stimuli}
\subsubsection{Procedure}
\subsection{Results}


\section{Study 4: Complexity bias in natural language} 
\subsection{\textbf{Experiment 4a}}
English
\subsection{Methods}
\subsubsection{Procedure}
\subsubsection{Participants}
\subsubsection{Stimuli}
\subsubsection{Procedure}
\subsection{Results}

\subsection{\textbf{Experiment 4b}}
google xling
\subsection{Methods}
\subsubsection{Procedure}
\subsubsection{Participants}
\subsubsection{Stimuli}
\subsubsection{Procedure}
\subsection{Results}

\section{General Discussion}
the causal chain is unclear 


\bibliographystyle{apacite2}
\bibliography{biblibrary}

\newpage
\theappendix 

\section{Appendix A: Materials}




\end{document}
